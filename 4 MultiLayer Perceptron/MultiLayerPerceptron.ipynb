{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lab Assignment Four: Multi-Layer Perceptron\n",
    "\n",
    "#### Luis Garduno\n",
    "\n",
    "Dataset : https://www.kaggle.com/muonneutrino/us-census-demographic-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------\n",
    "\n",
    "## 1. Load, Split, & Balance\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Loading Data & Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72718 entries, 0 to 74000\n",
      "Data columns (total 36 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           72718 non-null  int64  \n",
      " 1   State             72718 non-null  int64  \n",
      " 2   TotalPop          72718 non-null  int64  \n",
      " 3   Men               72718 non-null  int64  \n",
      " 4   Women             72718 non-null  int64  \n",
      " 5   Hispanic          72718 non-null  float64\n",
      " 6   White             72718 non-null  float64\n",
      " 7   Black             72718 non-null  float64\n",
      " 8   Native            72718 non-null  float64\n",
      " 9   Asian             72718 non-null  float64\n",
      " 10  Pacific           72718 non-null  float64\n",
      " 11  VotingAgeCitizen  72718 non-null  int64  \n",
      " 12  Income            72718 non-null  float64\n",
      " 13  IncomeErr         72718 non-null  float64\n",
      " 14  IncomePerCap      72718 non-null  float64\n",
      " 15  IncomePerCapErr   72718 non-null  float64\n",
      " 16  Poverty           72718 non-null  float64\n",
      " 17  ChildPoverty      72718 non-null  float64\n",
      " 18  Professional      72718 non-null  float64\n",
      " 19  Service           72718 non-null  float64\n",
      " 20  Office            72718 non-null  float64\n",
      " 21  Construction      72718 non-null  float64\n",
      " 22  Production        72718 non-null  float64\n",
      " 23  Drive             72718 non-null  float64\n",
      " 24  Carpool           72718 non-null  float64\n",
      " 25  Transit           72718 non-null  float64\n",
      " 26  Walk              72718 non-null  float64\n",
      " 27  OtherTransp       72718 non-null  float64\n",
      " 28  WorkAtHome        72718 non-null  float64\n",
      " 29  MeanCommute       72718 non-null  float64\n",
      " 30  Employed          72718 non-null  int64  \n",
      " 31  PrivateWork       72718 non-null  float64\n",
      " 32  PublicWork        72718 non-null  float64\n",
      " 33  SelfEmployed      72718 non-null  float64\n",
      " 34  FamilyWork        72718 non-null  float64\n",
      " 35  Unemployment      72718 non-null  float64\n",
      "dtypes: float64(29), int64(7)\n",
      "memory usage: 20.5 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into memory & save it to a pandas data frame.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/Datasets/acs2017_census_tract_data.csv\");\n",
    "\n",
    "# Zip State Name with ID number \n",
    "def_state = zip(df['State'].unique(), np.arange(52))\n",
    "\n",
    "# Remove any observations having missing data.\n",
    "df = df.dropna(axis=0, how='any')\n",
    "del df['County']\n",
    "\n",
    "# Encode State's as integers\n",
    "df['State'] = pd.factorize(df.State)[0]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Splitting the Dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAD4CAYAAADGmmByAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUIUlEQVR4nO3dfbDc1X3f8ffHiGA5Nn6SsFUJKhyrjgWTYJBVdZy2OLS1Yk8CbqGRp2OYlFgJxVN76j8CTCZ2p6MZPBOblklNggfKQxxj/AiNoSnGaTyZweBrSsKTqdVAjCINyIYCTmyI8Ld/7LmT1dXqaqVz99671vs1s7O//f5+Z/ccjtBHv4f9baoKSZKO1EuWugOSpOlmkEiSuhgkkqQuBokkqYtBIknqsmKpO7DYVq1aVevXr1/qbkjSVPnmN7/53apaPWrdURck69evZ2ZmZqm7IUlTJclfHmydh7YkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXY66b7ZLOtD6S768JJ/72OXvWpLP1cJyj0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXSZ2r60kJwI3AK8HfgRcXVX/JclHgPcBe9uml1XVba3NpcCFwIvAv6+qP2r1M4DrgJXAbcAHqqqSHNc+4wzge8AvV9VjkxqTJPVaqvuaweTubTbJPZJ9wIeq6s3AFuDiJBvbuiuq6rT2mA2RjcA24BRgK/CJJMe07a8CtgMb2mNrq18IPF1VbwSuAD46wfFIkkaYWJBU1Z6qurctPwc8DKydp8nZwE1V9XxVPQrsBDYnWQMcX1V3VVUx2AM5Z6jN9W35c8BZSbLwo5EkHcyinCNJsh54C3B3K70/yZ8nuTbJq1ttLfD4ULNdrba2Lc+t79emqvYBzwCvHfH525PMJJnZu3fv3NWSpA4TD5IkLwc+D3ywqp5lcJjqp4DTgD3Ax2Y3HdG85qnP12b/QtXVVbWpqjatXr368AYgSZrXRIMkybEMQuRTVfUFgKp6oqperKofAZ8ENrfNdwEnDjVfB+xu9XUj6vu1SbICeCXw1GRGI0kaZWJB0s5VXAM8XFUfH6qvGdrs3cADbflWYFuS45KczOCk+j1VtQd4LsmW9p7nA7cMtbmgLZ8LfLWdR5EkLZJJ/tTu24D3Avcnua/VLgPek+Q0BoegHgN+DaCqHkxyM/AQgyu+Lq6qF1u7i/i7y39vbw8YBNWNSXYy2BPZNsHxSJJGmFiQVNWfMvocxm3ztNkB7BhRnwFOHVH/IXBeRzclSZ38ZrskqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKnLxIIkyYlJ/jjJw0keTPKBVn9NkjuSfLs9v3qozaVJdiZ5JMk7hupnJLm/rbsySVr9uCSfafW7k6yf1HgkSaNNco9kH/ChqnozsAW4OMlG4BLgzqraANzZXtPWbQNOAbYCn0hyTHuvq4DtwIb22NrqFwJPV9UbgSuAj05wPJKkESYWJFW1p6rubcvPAQ8Da4GzgevbZtcD57Tls4Gbqur5qnoU2AlsTrIGOL6q7qqqAm6Y02b2vT4HnDW7tyJJWhyLco6kHXJ6C3A38Lqq2gODsAFOaJutBR4farar1da25bn1/dpU1T7gGeC1Iz5/e5KZJDN79+5doFFJkmARgiTJy4HPAx+sqmfn23REreapz9dm/0LV1VW1qao2rV69+lBdliQdhokGSZJjGYTIp6rqC638RDtcRXt+stV3AScONV8H7G71dSPq+7VJsgJ4JfDUwo9EknQwk7xqK8A1wMNV9fGhVbcCF7TlC4Bbhurb2pVYJzM4qX5PO/z1XJIt7T3Pn9Nm9r3OBb7azqNIkhbJigm+99uA9wL3J7mv1S4DLgduTnIh8B3gPICqejDJzcBDDK74uriqXmztLgKuA1YCt7cHDILqxiQ7GeyJbJvgeCRJI0wsSKrqTxl9DgPgrIO02QHsGFGfAU4dUf8hLYgkSUvDb7ZLkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6jJWkCQ54EelJEmC8fdIfjfJPUn+XZJXTbJDkqTpMlaQVNXPAf8GOBGYSfIHSf75RHsmSZoKY58jqapvA78J/AbwT4Erk3wryb+cVOckScvfuOdIfibJFcDDwM8Dv1hVb27LV0ywf5KkZW7FmNv9DvBJ4LKq+sFssap2J/nNifRMkjQVxg2SdwI/qKoXAZK8BHhpVf1NVd04sd5Jkpa9cc+RfAVYOfT6Za0mSTrKjRskL62q78++aMsvm0yXJEnTZNwg+eskp8++SHIG8IN5tpckHSXGPUfyQeCzSXa312uAX55IjyRJU2WsIKmqbyT5aeBNQIBvVdXfTrRnkqSpMO4eCcBbgfWtzVuSUFU3TKRXkqSpMe4XEm8Efhv4OQaB8lZg0yHaXJvkySQPDNU+kuSvktzXHu8cWndpkp1JHknyjqH6GUnub+uuTJJWPy7JZ1r97iTrD2fgkqSFMe4eySZgY1XVYbz3dQy+yDh3r+WKqvrt4UKSjcA24BTg7wFfSfIP2vdWrgK2A18HbgO2ArcDFwJPV9Ubk2wDPornbSRp0Y171dYDwOsP542r6mvAU2NufjZwU1U9X1WPAjuBzUnWAMdX1V0txG4Azhlqc31b/hxw1uzeiiRp8Yy7R7IKeCjJPcDzs8Wq+qUj+Mz3JzkfmAE+VFVPA2sZ7HHM2tVqf9uW59Zpz4+3fuxL8gzwWuC7R9AnSdIRGjdIPrJAn3cV8J+Aas8fA/4tgyvB5qp56hxi3X6SbGdweIyTTjrp8HosSZrXuL9H8ifAY8CxbfkbwL2H+2FV9URVvVhVP2JwE8jNbdUuBr91MmsdsLvV142o79cmyQrglRzkUFpVXV1Vm6pq0+rVqw+325KkeYx71db7GJyH+L1WWgt86XA/rJ3zmPVuBudeAG4FtrUrsU4GNgD3VNUe4LkkW9r5j/OBW4baXNCWzwW+epgXA0iSFsC4h7YuZrD3cDcMfuQqyQnzNUjyaeBMYFWSXcCHgTOTnMbgENRjwK+193swyc3AQ8A+4OLZOw0DFzG4Amwlg6u1bm/1a4Abk+xksCeybcyxSJIW0LhB8nxVvTB7UVQ7lDTvv/6r6j0jytfMs/0OYMeI+gxw6oj6D4Hz5u+2JGnSxr3890+SXAasbL/V/lngv0+uW5KkaTFukFwC7AXuZ3A46jYGv98uSTrKjXvTxtmrrD452e5IkqbNWEGS5FFGnBOpqjcseI8kSVPlcO61NeulDE5yv2bhuyNJmjbjfiHxe0OPv6qq/wz8/GS7JkmaBuMe2jp96OVLGOyhvGIiPZIkTZVxD219bGh5H4MvE/7rBe+NJGnqjHvV1tsn3RFJ0nQa99DWf5hvfVV9fGG6I0maNodz1dZbGdwoEeAXga/Rfg9EknT0Opwftjq9qp6DwW+vA5+tql+dVMckSdNh3FuknAS8MPT6BWD9gvdGkjR1xt0juRG4J8kXGXzD/d0Mfj9dknSUG/eqrR1Jbgf+cSv9SlX978l1S5I0LcbdIwF4GfBsVf23JKuTnFxVj06qY8vR+ku+vGSf/djl71qyz5ak+Yz7U7sfBn4DuLSVjgV+f1KdkiRNj3FPtr8b+CXgrwGqajfeIkWSxPhB8kJVFe1W8kl+cnJdkiRNk3GD5OYkvwe8Ksn7gK/gj1xJkhjjZHuSAJ8Bfhp4FngT8FtVdceE+yZJmgKHDJKqqiRfqqozAMNDkrSfcQ9tfT3JWyfaE0nSVBr3eyRvB349yWMMrtwKg52Vn5lUxyRJ02HeIElyUlV9B/iFReqPJGnKHGqP5EsM7vr7l0k+X1X/ahH6JEmaIoc6R5Kh5TdMsiOSpOl0qCCpgyxLkgQc+tDWzyZ5lsGeycq2DH93sv34ifZOkrTszRskVXXMYnVEkjSdxv0eyWFLcm2SJ5M8MFR7TZI7kny7Pb96aN2lSXYmeSTJO4bqZyS5v627sn3TniTHJflMq9+dZP2kxiJJOriJBQlwHbB1Tu0S4M6q2gDc2V6TZCOwDTiltflEktm9oauA7cCG9ph9zwuBp6vqjcAVwEcnNhJJ0kFNLEiq6mvAU3PKZwPXt+XrgXOG6jdV1fPtx7J2ApuTrAGOr6q72t2Hb5jTZva9PgecNbu3IklaPJPcIxnldVW1B6A9n9Dqa4HHh7bb1Wpr2/Lc+n5tqmof8Azw2lEfmmR7kpkkM3v37l2goUiSYPGD5GBG7UnUPPX52hxYrLq6qjZV1abVq1cfYRclSaMsdpA80Q5X0Z6fbPVdwIlD260Ddrf6uhH1/dokWQG8kgMPpUmSJmyxg+RW4IK2fAFwy1B9W7sS62QGJ9XvaYe/nkuypZ3/OH9Om9n3Ohf4ajuPIklaROPe/fewJfk0cCawKsku4MPA5Qx+bfFC4DvAeQBV9WCSm4GHgH3AxVX1YnurixhcAbYSuL09AK4Bbkyyk8GeyLZJjUWSdHATC5Kqes9BVp11kO13ADtG1GeAU0fUf0gLIknS0lkuJ9slSVPKIJEkdTFIJEldDBJJUheDRJLUxSCRJHUxSCRJXQwSSVIXg0SS1GVi32yXeq2/5MtL8rmPXf6uJflcaVq5RyJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuSxIkSR5Lcn+S+5LMtNprktyR5Nvt+dVD21+aZGeSR5K8Y6h+RnufnUmuTJKlGI8kHc2Wco/k7VV1WlVtaq8vAe6sqg3Ane01STYC24BTgK3AJ5Ic09pcBWwHNrTH1kXsvySJ5XVo62zg+rZ8PXDOUP2mqnq+qh4FdgKbk6wBjq+qu6qqgBuG2kiSFslSBUkB/zPJN5Nsb7XXVdUegPZ8QquvBR4farur1da25bn1AyTZnmQmyczevXsXcBiSpBVL9Llvq6rdSU4A7kjyrXm2HXXeo+apH1isuhq4GmDTpk0jt5EkHZkl2SOpqt3t+Ungi8Bm4Il2uIr2/GTbfBdw4lDzdcDuVl83oi5JWkSLHiRJfjLJK2aXgX8BPADcClzQNrsAuKUt3wpsS3JckpMZnFS/px3+ei7Jlna11vlDbSRJi2QpDm29Dvhiu1J3BfAHVfU/knwDuDnJhcB3gPMAqurBJDcDDwH7gIur6sX2XhcB1wErgdvbQ5K0iBY9SKrqL4CfHVH/HnDWQdrsAHaMqM8Apy50HyVJ41tOl/9KkqaQQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSepikEiSuhgkkqQuBokkqYtBIknqYpBIkroYJJKkLgaJJKmLQSJJ6mKQSJK6GCSSpC4GiSSpi0EiSeoy9UGSZGuSR5LsTHLJUvdHko42Ux0kSY4B/ivwC8BG4D1JNi5tryTp6DLVQQJsBnZW1V9U1QvATcDZS9wnSTqqpKqWug9HLMm5wNaq+tX2+r3AP6yq98/Zbjuwvb18E/DIEX7kKuC7R9h2uXEsy8+PyzjAsSxXPWP5+1W1etSKFUfen2UhI2oHJGNVXQ1c3f1hyUxVbep9n+XAsSw/Py7jAMeyXE1qLNN+aGsXcOLQ63XA7iXqiyQdlaY9SL4BbEhycpKfALYBty5xnyTpqDLVh7aqal+S9wN/BBwDXFtVD07wI7sPjy0jjmX5+XEZBziW5WoiY5nqk+2SpKU37Ye2JElLzCCRJHUxSEY41G1XMnBlW//nSU5fin6OY4yxnJnkmST3tcdvLUU/DyXJtUmeTPLAQdZP05wcaizTMicnJvnjJA8neTDJB0ZsMxXzMuZYlv28JHlpknuS/Fkbx38csc3Cz0lV+Rh6MDhp/3+BNwA/AfwZsHHONu8EbmfwPZYtwN1L3e+OsZwJ/OFS93WMsfwT4HTggYOsn4o5GXMs0zIna4DT2/IrgP8zxf+vjDOWZT8v7b/zy9vyscDdwJZJz4l7JAca57YrZwM31MDXgVclWbPYHR3Dj80tZKrqa8BT82wyLXMyzlimQlXtqap72/JzwMPA2jmbTcW8jDmWZa/9d/5+e3lse8y9omrB58QgOdBa4PGh17s48A/UONssB+P28x+1XeHbk5yyOF1bcNMyJ+OaqjlJsh54C4N/AQ+bunmZZywwBfOS5Jgk9wFPAndU1cTnZKq/RzIh49x2ZaxbsywD4/TzXgb30Pl+kncCXwI2TLpjEzAtczKOqZqTJC8HPg98sKqenbt6RJNlOy+HGMtUzEtVvQicluRVwBeTnFpVw+fjFnxO3CM50Di3XZmWW7Mcsp9V9ezsrnBV3QYcm2TV4nVxwUzLnBzSNM1JkmMZ/MX7qar6wohNpmZeDjWWaZoXgKr6f8D/ArbOWbXgc2KQHGic267cCpzfrn7YAjxTVXsWu6NjOORYkrw+SdryZgZ/Jr636D3tNy1zckjTMietj9cAD1fVxw+y2VTMyzhjmYZ5SbK67YmQZCXwz4BvzdlswefEQ1tz1EFuu5Lk19v63wVuY3Dlw07gb4BfWar+zmfMsZwLXJRkH/ADYFu1SzuWkySfZnDVzKoku4APMziROFVzAmONZSrmBHgb8F7g/nZMHuAy4CSYunkZZyzTMC9rgOsz+NG/lwA3V9UfTvrvL2+RIknq4qEtSVIXg0SS1MUgkSR1MUgkSV0MEklSF4NEktTFIJEkdfn/1zWBex0TqsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Divide 'ChildPoverty' into 4 classes\n",
    "ChildPovertyClass = ['Okay', 'U.S. Poverty Rate', 'Very Poor', 'Extremely Poor']\n",
    "\n",
    "# this creates a new variable\n",
    "df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.00,0.17,0.28,0.65,1.0],labels=False)\n",
    "\n",
    "df['ChildPoverty_Class'].plot(kind='hist',alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources were used to gain an understanding of where the national poverty was during 2017.\n",
    "- __[Child Poverty in America 2017: State Analysis](https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-State-Fact-Sheet.pdf)__\n",
    "\n",
    "- __[Child Poverty in America 2017: National Analysis](https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-National-Fact-Sheet.pdf)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set \n",
      "   - Data Shape: (58174, 36) \n",
      "   - Target Shape: (58174,)\n",
      "\n",
      "Testing Set \n",
      "   - Data Shape: (14544, 36) \n",
      "   - Target Shape: (14544,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume I'm equally interested in the classification performance for each class in the dataset.\n",
    "\n",
    "# Create X data & y target dataframe's\n",
    "if 'ChildPoverty' in df:\n",
    "    y = df['ChildPoverty_Class'].values\n",
    "    del df['ChildPoverty_Class']\n",
    "    X = df.to_numpy()\n",
    "\n",
    "# Notice we only perform fit_transform on X\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Divide the data: 80% Training & 20% Testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"Training Set\", \"\\n   - Data Shape:\",X_train.shape,\"\\n   - Target Shape:\",y_train.shape)\n",
    "print(\"\\nTesting Set\",\"\\n   - Data Shape:\",X_test.shape ,\"\\n   - Target Shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Balancing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "             Okay       1.00      0.98      0.99      2447\n",
      "U.S. Poverty Rate       0.85      1.00      0.92      1626\n",
      "        Very Poor       1.00      0.96      0.98      5450\n",
      "   Extremely Poor       1.00      1.00      1.00      5021\n",
      "\n",
      "         accuracy                           0.98     14544\n",
      "        macro avg       0.96      0.98      0.97     14544\n",
      "     weighted avg       0.98      0.98      0.98     14544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', RandomizedPCA(n_components=35,random_state=1)),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced', random_state=1, max_iter=500))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=ChildPovertyClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Acc: 0.979\n",
      "Fold: 2, Acc: 0.978\n",
      "Fold: 3, Acc: 0.973\n",
      "Fold: 4, Acc: 0.982\n",
      "Fold: 5, Acc: 0.980\n",
      "Fold: 6, Acc: 0.980\n",
      "Fold: 7, Acc: 0.978\n",
      "Fold: 8, Acc: 0.977\n",
      "Fold: 9, Acc: 0.977\n",
      "Fold: 10, Acc: 0.979\n",
      "\n",
      "CV accuracy: 0.978 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Acc: %.3f' % (k+1, score))\n",
    "\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split 80/20. Balance 80, argue whether 20 should be balanced\n",
    "- Explain reasoning for selecting this method for balancing the dataset. \n",
    "- Should balancing of the dataset be done for both the training & testing set? Explain.\n",
    "\n",
    "If you do it to the test set, you're misrepresenting the reality of the dataset.\n",
    "We do it on the training set so that our model doesn't overly classify just one child poverty rate.\n",
    "But on the testing set we don't want to introduce new data or massage the data on a test set.\n",
    "For the integrity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "------------------------\n",
    "\n",
    "## 2. Pre-Processing\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Quantifying Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Two-Layer Perceptron network example (Mini-Batching & Glorant) provided during lecture,\n",
    "# Let's quantify performance using accuracy. We don't normalize or one-hot encode the data just yet.\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted it only has internal classes to be used by classes that will subclass it\n",
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001,\n",
    "                 random_state=None, alpha=0.0,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, activation='sigmoid',\n",
    "                 obj_func='quadratic'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.activation = activation\n",
    "        self.obj_func = obj_func\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\" Encode labels into one-hot representation \"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        hidden_layers = self.n_hidden\n",
    "\n",
    "        if not hasattr(hidden_layers, \"__iter__\"): hidden_layers = [hidden_layers]\n",
    "\n",
    "        hidden_layers = list(hidden_layers)\n",
    "        hidden_layers += [self.n_output_]\n",
    "\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        weights = []\n",
    "        w1 = []\n",
    "        n_layers = len(hidden_layers)\n",
    "\n",
    "        for i, n_hidden in enumerate(hidden_layers):\n",
    "\n",
    "            if self.activation == 'sigmoid' or self.activation == 'linear':\n",
    "                w1_num_elems = (self.n_features_ + 1) * n_hidden\n",
    "                w1 = np.random.uniform(-1.0, 1.0, size=w1_num_elems)\n",
    "                w1 = w1.reshape(n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "            elif self.activation == 'relu' or self.activation == 'silu':\n",
    "                if i == n_layers - 1: init_bound = np.sqrt(0.5 / (n_hidden + self.n_features_ + 1))\n",
    "                else: init_bound = np.sqrt(6. / (n_hidden + self.n_features_ + 1))\n",
    "\n",
    "                w1 = np.random.uniform(-init_bound, init_bound, (n_hidden, self.n_features_ + 1))\n",
    "                w1[:, :1] = 0\n",
    "                \n",
    "            self.n_features_ = n_hidden\n",
    "            weights += [w1]\n",
    "            \n",
    "        return weights\n",
    "\n",
    "    # @staticmethod\n",
    "    def _activation(self, z):\n",
    "        if self.activation == 'linear': return z\n",
    "        if self.activation == 'sigmoid': return expit(z)\n",
    "        if self.activation == 'relu': return np.maximum(0, z.copy())\n",
    "        if self.activation == 'silu':\n",
    "            z = z.copy() * expit(z.copy())\n",
    "            return z\n",
    "        \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\" Add bias unit (column or row of 1s) to array at index 0 \"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        w = 0\n",
    "        for w_j in weights:\n",
    "            w += np.mean(w_j[:, 1:] ** 2)\n",
    "            \n",
    "        return (lambda_/2.0) * np.sqrt(w)\n",
    "        \n",
    "        #return (lambda_/2.0) * np.sqrt(np.mean(w1[:, 1:] ** 2) + np.mean(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self, a3, Y_enc, weights):\n",
    "        \"\"\" Get the objective function value \"\"\"\n",
    "        if self.obj_func == 'quadratic':\n",
    "            cost = np.mean((Y_enc - a3) ** 2)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            #cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3[-1])+(1-Y_enc)*np.log(1-a3[-1]))))\n",
    "            #cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3)+(1-Y_enc)*np.log(1-a3))))\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3)+(1-Y_enc)*np.log(1-a3))))\n",
    "        \n",
    "        L2_term = self._L2_reg(self.l2_C, weights)\n",
    "        return cost + L2_term\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \"\"\" Compute feedforward step \"\"\"\n",
    "        a_i = []\n",
    "        z_i = []\n",
    "        z = None\n",
    "        for idx, W in enumerate(weights):\n",
    "            if idx == 0:\n",
    "                a1 = self._add_bias_unit(X.T, how='row')\n",
    "            else:\n",
    "                a1 = self._activation(z)\n",
    "                a1 = self._add_bias_unit(a1, how='row')\n",
    "            z1 = W @ a1\n",
    "            a_i += [a1]\n",
    "            z_i += [z1]\n",
    "            z = z1\n",
    "        a_out = self._activation(z)\n",
    "        a_i += [a_out]\n",
    "        \n",
    "        return a_i, z_i\n",
    "\n",
    "    def _get_gradient(self, a, z, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        gradients = []\n",
    "        A_output = a[-1]\n",
    "        \n",
    "        if self.obj_func == 'quadratic':\n",
    "            V_last = -2 * (Y_enc - A_output) * A_output * (1-A_output)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            V_last = (A_output - Y_enc)\n",
    "        \n",
    "        i = 1\n",
    "        for A, W in zip(a[:-1][::-1], weights[::-1]):\n",
    "            if i == 1:\n",
    "                grad = V_last @ A.T         \n",
    "                V1 = (W.T @ V_last)    \n",
    "            else:\n",
    "                grad = V_last[1:,:] @ A.T   \n",
    "                if i != len(weights):\n",
    "                    V1 = (W.T @ V_last[1:, :])\n",
    "            \n",
    "            if len(weights)-1-i >= 0:\n",
    "\n",
    "                if self.activation == 'linear':\n",
    "                    V1 = V1\n",
    "\n",
    "                elif self.activation == 'sigmoid':\n",
    "                    V1 = A * (1 - A) * V1\n",
    "\n",
    "                elif self.activation == 'relu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    V1[Z1_with_bias<=0] = 0\n",
    "\n",
    "                elif self.activation == 'silu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    sig = expit(Z1_with_bias.copy())\n",
    "                    V1 = (A + (1-A) * sig * V1)\n",
    "                V_last = V1\n",
    "\n",
    "            grad[:, 1:] += W[:, 1:] * self.l2_C\n",
    "            gradients.insert(0, grad)\n",
    "            \n",
    "            i += 1\n",
    "        return gradients\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y)\n",
    "\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = y_enc.shape[0]\n",
    "        self.weights= self._initialize_weights()\n",
    "        \n",
    "        W_prev_list = []\n",
    "        for W in self.weights:\n",
    "            delta_W_prev = np.zeros(W.shape)\n",
    "            W_prev_list += [delta_W_prev]\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "                a, z = self._feedforward(X_data[idx], self.weights)\n",
    "\n",
    "                cost = self._cost(a[-1], Y_enc[:, idx], self.weights)\n",
    "                mini_cost.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradients = self._get_gradient(a, z, Y_enc[:,idx], self.weights)\n",
    "\n",
    "                delta_W_list = []\n",
    "                for gradient in gradients:\n",
    "                    delta_W = self.eta * gradient\n",
    "                    delta_W_list += [delta_W]\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def get_params(self,deep=True):\n",
    "        return {\"n_hidden\" : self.n_hidden,\n",
    "                \"activation\" : self.activation,\n",
    "                \"obj_func\" : self.obj_func}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels \"\"\"\n",
    "        A3, _, = self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(A3[-1], axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n",
    "params = dict(n_hidden=(50,50,50,),\n",
    "              C=0.1,\n",
    "              epochs=300,\n",
    "              eta=0.001,\n",
    "              random_state=1,\n",
    "              alpha=0.001,\n",
    "              decrease_const=0.0001,\n",
    "              shuffle=True,\n",
    "              minibatches=50,\n",
    "              activation='relu',\n",
    "              obj_func='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 300/300"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.17663641364136415\n",
      "CPU times: user 4min 25s, sys: 7.49 s, total: 4min 33s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "mlp = MultiLayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=10)\n",
    "yhat = mlp.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now normalize the continuous numeric feature data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Normalizing & One Hot Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now normalize the continuous numeric feature data AND one hot encode the categorical data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Comparing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of the three models I just trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any meaningful differences in performance?\n",
    "- In my own words,explain why these models have (or don't have) different performances.\n",
    "    - _Use one-hot encoding & normalization on the dataset for the remainder of this lab assignment._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Three-layer Support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sup for a third layer in the multi-layer perceptron.\n",
    "# Add sup for saving(& plotting after training is completed) the avg magnitude of the gradient for each layer, for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the performance of the model & graph the magnitudes for each layer Vs. the # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Four-layer Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Five-layer Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Adaptive Learning Technique w/ five-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an adaptive learning technique that was discussed in lecture & use it on the five layer network.\n",
    "# Don't use AdaM for the adaptive learning technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the performance of this model w/ & w/o the adaptive learning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------\n",
    "\n",
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Adaptive Momentum\n",
    "- 5000 level student: I have free reign to provide additional analyses.\n",
    "- One idea: Implement adaptive momentum (AdaM) in the five layer neural network & quantify the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------------\n",
    "\n",
    "### Reference\n",
    "\n",
    "Kaggle. US Census Demographics. https://www.kaggle.com/muonneutrino/us-census-demographic-data/data (Accessed 10-24-2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
