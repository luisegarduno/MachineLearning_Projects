{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lab Assignment Four: Multi-Layer Perceptron\n",
    "\n",
    "#### Luis Garduno\n",
    "\n",
    "Dataset : https://www.kaggle.com/muonneutrino/us-census-demographic-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------\n",
    "\n",
    "## 1. Load, Split, & Balance\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Loading Data & Adjustments  (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72718 entries, 0 to 74000\n",
      "Data columns (total 36 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           72718 non-null  int64  \n",
      " 1   State             72718 non-null  int64  \n",
      " 2   TotalPop          72718 non-null  int64  \n",
      " 3   Men               72718 non-null  int64  \n",
      " 4   Women             72718 non-null  int64  \n",
      " 5   Hispanic          72718 non-null  float64\n",
      " 6   White             72718 non-null  float64\n",
      " 7   Black             72718 non-null  float64\n",
      " 8   Native            72718 non-null  float64\n",
      " 9   Asian             72718 non-null  float64\n",
      " 10  Pacific           72718 non-null  float64\n",
      " 11  VotingAgeCitizen  72718 non-null  int64  \n",
      " 12  Income            72718 non-null  float64\n",
      " 13  IncomeErr         72718 non-null  float64\n",
      " 14  IncomePerCap      72718 non-null  float64\n",
      " 15  IncomePerCapErr   72718 non-null  float64\n",
      " 16  Poverty           72718 non-null  float64\n",
      " 17  ChildPoverty      72718 non-null  float64\n",
      " 18  Professional      72718 non-null  float64\n",
      " 19  Service           72718 non-null  float64\n",
      " 20  Office            72718 non-null  float64\n",
      " 21  Construction      72718 non-null  float64\n",
      " 22  Production        72718 non-null  float64\n",
      " 23  Drive             72718 non-null  float64\n",
      " 24  Carpool           72718 non-null  float64\n",
      " 25  Transit           72718 non-null  float64\n",
      " 26  Walk              72718 non-null  float64\n",
      " 27  OtherTransp       72718 non-null  float64\n",
      " 28  WorkAtHome        72718 non-null  float64\n",
      " 29  MeanCommute       72718 non-null  float64\n",
      " 30  Employed          72718 non-null  int64  \n",
      " 31  PrivateWork       72718 non-null  float64\n",
      " 32  PublicWork        72718 non-null  float64\n",
      " 33  SelfEmployed      72718 non-null  float64\n",
      " 34  FamilyWork        72718 non-null  float64\n",
      " 35  Unemployment      72718 non-null  float64\n",
      "dtypes: float64(29), int64(7)\n",
      "memory usage: 20.5 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into memory & save it to a pandas data frame.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/Datasets/acs2017_census_tract_data.csv\");\n",
    "\n",
    "# Zip State Name with ID number \n",
    "def_state = zip(df['State'].unique(), np.arange(52))\n",
    "\n",
    "# Remove any observations having missing data.\n",
    "df = df.dropna(axis=0, how='any')\n",
    "del df['County']\n",
    "\n",
    "# Encode State's as integers\n",
    "df['State'] = pd.factorize(df.State)[0]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Splitting the Dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set \n",
      "   - Data Shape: (58174, 36) \n",
      "   - Target Shape: (58174,)\n",
      "\n",
      "Testing Set \n",
      "   - Data Shape: (14544, 36) \n",
      "   - Target Shape: (14544,)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "scaler = StandardScaler()\n",
    "\n",
    "ChildPoverty_labels = ['Okay', 'U.S. Poverty Rate', 'Very Poor', 'Extremely Poor']\n",
    "df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.00,0.17,0.28,0.65,1.0],labels=False) # this creates a new variable\n",
    "ChildPovertyClass = ChildPoverty_labels\n",
    "\n",
    "# 0.0 - 0.17 : 'Okay' | 0.17 - 0.28 : 'U.S. Poverty Rate' | 0.28 - 0.65 : 'Very Poor' | 0.65 - 1.00 : 'Extremely Poor'\n",
    "#df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.0,0.17,0.28,0.65,1.0],labels=False) # this creates a new variable\n",
    "#ChildPovertyClass = df['ChildPoverty_Class']\n",
    "\n",
    "df['ChildPoverty_Class'].plot(kind='hist',alpha=1.0)   # Blue\n",
    "plt.show()\n",
    "\n",
    "# Create X data & y target dataframe's\n",
    "if 'ChildPoverty' in df:\n",
    "    y = df['ChildPoverty_Class'].values\n",
    "    del df['ChildPoverty_Class']\n",
    "    X = df.to_numpy()\n",
    "\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# Divide the data: 80% Training & 20% Testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"Training Set\", \"\\n   - Data Shape:\",X_train.shape,\"\\n   - Target Shape:\",y_train.shape)\n",
    "print(\"\\nTesting Set\",\"\\n   - Data Shape:\",X_test.shape ,\"\\n   - Target Shape:\",y_test.shape)\n",
    "\n",
    "#df['ChildPoverty_Class'].plot(kind='hist',alpha=1.0)   # Blue\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Assume I'm equally interested in the classification performance for each class in the dataset.\n",
    "# Split the dataset into 80% for training and 20% for testing.\n",
    "\n",
    "# Option 1.\n",
    "# Figure out how I want to Quantize it  --> Quantize it --> Split it into 80/20\n",
    "\n",
    "# Option 2.\n",
    "# If introducing variables (Overlap Sampling) --> Split into 80/20 --> Introduce variables ONLY in dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "             Okay       1.00      0.98      0.99      2447\n",
      "U.S. Poverty Rate       0.85      1.00      0.92      1626\n",
      "        Very Poor       1.00      0.96      0.98      5450\n",
      "   Extremely Poor       1.00      1.00      1.00      5021\n",
      "\n",
      "         accuracy                           0.98     14544\n",
      "        macro avg       0.96      0.98      0.97     14544\n",
      "     weighted avg       0.98      0.98      0.98     14544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import utils\n",
    "from sklearn import metrics, svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', RandomizedPCA(n_components=35,random_state=1)),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced', random_state=1, max_iter=500))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=ChildPovertyClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Acc: 0.979\n",
      "Fold: 2, Acc: 0.978\n",
      "Fold: 3, Acc: 0.973\n",
      "Fold: 4, Acc: 0.982\n",
      "Fold: 5, Acc: 0.980\n",
      "Fold: 6, Acc: 0.980\n",
      "Fold: 7, Acc: 0.978\n",
      "Fold: 8, Acc: 0.977\n",
      "Fold: 9, Acc: 0.977\n",
      "Fold: 10, Acc: 0.979\n",
      "\n",
      "CV accuracy: 0.978 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Acc: %.3f' % (k+1, score))\n",
    "\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources were used to gain an understanding of where the national poverty was during 2017.\n",
    "\n",
    "- <a href=\"https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-State-Fact-Sheet.pdf\" target=\"_top\">\n",
    "    <b>Child Poverty in America 2017: State Analysis</b>\n",
    "  </a>\n",
    "- <a href=\"https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-National-Fact-Sheet.pdf\" target=\"_top\">\n",
    "    <b>Child Poverty in America 2017: National Analysis</b>\n",
    "  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Balancing the Data(15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Balance the dataset so that about the same # of instances are within each class (divide into 4 classes by Child Poverty)\n",
    "# Once I divide this up by child poverty(4 classes), each class has about the same # of examples for each of those counties\n",
    "\n",
    "# Split 80/20. Balance 80, argue whether 20 should be balanced\n",
    "\n",
    "# Choose a method for balancing the dataset (Quantiles or Overlap Sampling(Only do it on the training set))\n",
    "# An option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into 4 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Explain reasoning for selecting this method for balancing the dataset. \n",
    "\n",
    "\n",
    "- Should balancing of the dataset be done for both the training & testing set? Explain.\n",
    "\n",
    "If you do it to the test set, you're misrepresenting the reality of the dataset.\n",
    "We do it on the training set so that our model doesn't overly classify just one child poverty rate.\n",
    "But on the testing set we don't want to introduce new data or massage the data on a test set.\n",
    "For the integrity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "------------------------\n",
    "\n",
    "## 2. Pre-Processing\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Quantifying Performance (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the example (try & use Glorant & mini-batching) two-layer perceptron network from the class example & quantify performance using accuracy.\n",
    "# Don't normalize or one-hot encode the data (not yet).\n",
    "import sys\n",
    "from scipy.special import expit\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted it only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\" Encode labels into one-hot representation \"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "\n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\" Initialize weights with small random numbers. \"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\" Use scipy.special.expit to avoid overflow \"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\" Add bias unit (column or row of 1s) to array at index 0 \"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\" Compute L2-regularization cost \"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        \"\"\" Get the objective function value \"\"\"\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\" Compute feedforward step \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity\n",
    "\n",
    "        grad2 = V2 @ A2.T # no bias on final layer\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# just start with the vectorized version and minibatch\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True,minibatches=1, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "\n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "\n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "\n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "\n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc[:, idx], W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                self.W1 -= (delta_W1 + (self.alpha * delta_W1_prev))\n",
    "                self.W2 -= (delta_W2 + (self.alpha * delta_W2_prev))\n",
    "                delta_W1_prev, delta_W2_prev = delta_W1, delta_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "class TLPGlorot(TLPMiniBatch):\n",
    "    def __init__(self, dropout=0.25, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "\n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients between\n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1))\n",
    "        W2[:,:1] = 0\n",
    "\n",
    "        return W1, W2\n",
    "\n",
    "    def fit(self, X, y, print_progress=0, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        delta_W1_prev = np.zeros(self.W1.shape)\n",
    "        delta_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            # adding dropout neurons\n",
    "            W1 = self.W1.copy()\n",
    "            W2 = self.W2.copy()\n",
    "\n",
    "            if self.dropout>0.0:\n",
    "\n",
    "                # randomly select half of the neurons\n",
    "                idx_dropout = np.random.permutation(W1.shape[0])\n",
    "                #idx_other_half = idx_dropout[:int(W1.shape[0]*self.dropout)]\n",
    "                idx_dropout = idx_dropout[int(W1.shape[0]*(1-self.dropout)):] #drop half\n",
    "\n",
    "                idx_dropout = np.sort(idx_dropout)\n",
    "                idx_W2_withbias = np.hstack(([0],(idx_dropout+1)))\n",
    "                W1 = W1[idx_dropout,:]# get rid of rows\n",
    "                W2 = W2[:,idx_W2_withbias]# get rid of extra columns\n",
    "                delta_W1_prev_dropout = delta_W1_prev[idx_dropout,:]\n",
    "                delta_W2_prev_dropout = delta_W2_prev[:,idx_W2_withbias]\n",
    "            else:\n",
    "                delta_W1_prev_dropout = delta_W1_prev\n",
    "                delta_W2_prev_dropout = delta_W2_prev\n",
    "\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx], W1, W2)\n",
    "\n",
    "                cost = self._cost(A3,Y_enc[:, idx],W1,W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, Y_enc=Y_enc[:, idx], W1=W1,W2=W2)\n",
    "\n",
    "                delta_W1, delta_W2 = self.eta * grad1, self.eta * grad2\n",
    "                W1 -= (delta_W1 + (self.alpha * delta_W1_prev_dropout))\n",
    "                W2 -= (delta_W2 + (self.alpha * delta_W2_prev_dropout))\n",
    "                delta_W1_prev_dropout, delta_W2_prev_dropout = delta_W1, delta_W2\n",
    "\n",
    "            if self.dropout>0.0:\n",
    "                # now append the learned weights back into the original matrices\n",
    "                self.W1[idx_dropout,:] = W1\n",
    "                self.W2[:,idx_W2_withbias] = W2\n",
    "                delta_W1_prev[idx_dropout,:] = delta_W1_prev_dropout\n",
    "                delta_W2_prev[:,idx_W2_withbias] = delta_W2_prev_dropout\n",
    "            else:\n",
    "                # don't eliminate any neurons\n",
    "                self.W1 = W1\n",
    "                self.W2 = W2\n",
    "                delta_W1_prev = delta_W1_prev_dropout\n",
    "                delta_W2_prev = delta_W2_prev_dropout\n",
    "\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            self.cost_.append(mini_cost) # only uses dropped samples, so more noise\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        return self"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "\n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "\n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "\n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10/10"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.23 s\n",
      "Wall time: 4.18 s\n",
      "=================\n",
      "Traditional :\n",
      "Resubstitution acc: 0.9065733832983808\n",
      "Validation acc: 0.903052805280528\n",
      "=================\n",
      "Glorot Initial :\n",
      "Resubstitution acc: 0.9147213531818338\n",
      "Validation acc: 0.914535203520352\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = {'n_hidden':30,'C':0.01, 'epochs':10, 'eta':0.001, 'alpha':0.001,\n",
    "        'decrease_const':1e-5, 'minibatches':50,'shuffle':True,'random_state':1}\n",
    "\n",
    "nn_tradit = TLPMiniBatch(**vals)\n",
    "nn_better = TLPGlorot(**vals, dropout=False)\n",
    "\n",
    "%time nn_tradit.fit(X_train, y_train, print_progress=1)\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn_tradit,X_train,y_train,X_test,y_test,title=\"Traditional\",color=\"red\")\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Glorot Initial\",color=\"blue\")\n",
    "plt.show()\n",
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Normalizing Data (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now normalize the continuous numeric feature data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Normalizing & One Hot Encoding the Data (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now normalize the continuous numeric feature data AND one hot encode the categorical data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Comparing the Models (40 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare the performance of the three models I just trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any meaningful differences in performance?\n",
    "- In my own words,explain why these models have (or don't have) different performances.\n",
    "    - _Use one-hot encoding & normalization on the dataset for the remainder of this lab assignment._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Three-layer Support (50 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add sup for a third layer in the multi-layer perceptron.\n",
    "# Add sup for saving(& plotting after training is completed) the avg magnitude of the gradient for each layer, for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantify the performance of the model & graph the magnitudes for each layer Vs. the # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Four-layer Support (60 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat the previous step, adding support for a fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Five-layer Support (70 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat the previous step, adding support for a fifth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Adaptive Learning Technique w/ five-layer network (90 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement an adaptive learning technique that was discussed in lecture & use it on the five layer network.\n",
    "# Don't use AdaM for the adaptive learning technique. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Compare the performance of this model w/ & w/o the adaptive learning strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------\n",
    "\n",
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Adaptive Momentum (100 points)\n",
    "- 5000 level student: I have free reign to provide additional analyses.\n",
    "- One idea: Implement adaptive momentum (AdaM) in the five layer neural network & quantify the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------------\n",
    "\n",
    "### Reference\n",
    "\n",
    "Kaggle. US Census Demographics. https://www.kaggle.com/muonneutrino/us-census-demographic-data/data (Accessed 10-24-2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}