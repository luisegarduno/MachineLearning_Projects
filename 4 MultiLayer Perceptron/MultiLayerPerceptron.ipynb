{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lab Assignment Four: Multi-Layer Perceptron\n",
    "\n",
    "#### Luis Garduno\n",
    "\n",
    "Dataset : https://www.kaggle.com/muonneutrino/us-census-demographic-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------\n",
    "\n",
    "## 1. Load, Split, & Balance\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Loading Data & Adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into memory & save it to a pandas data frame.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/Datasets/acs2017_census_tract_data.csv\");\n",
    "\n",
    "# Zip State Name with ID number \n",
    "def_state = zip(df['State'].unique(), np.arange(52))\n",
    "\n",
    "# Remove any observations having missing data.\n",
    "df = df.dropna(axis=0, how='any')\n",
    "del df['County']\n",
    "\n",
    "# Encode State's as integers\n",
    "df['State'] = pd.factorize(df.State)[0]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Splitting the Dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Divide 'ChildPoverty' into 4 classes\n",
    "ChildPovertyClass = ['Okay', 'U.S. Poverty Rate', 'Very Poor', 'Extremely Poor']\n",
    "\n",
    "# this creates a new variable\n",
    "df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.00,0.17,0.28,0.65,1.0],labels=False)\n",
    "\n",
    "df['ChildPoverty_Class'].plot(kind='barh',alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources were used to gain an understanding of where the national poverty was during 2017.\n",
    "- __[Child Poverty in America 2017: State Analysis](https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-State-Fact-Sheet.pdf)__\n",
    "\n",
    "- __[Child Poverty in America 2017: National Analysis](https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-National-Fact-Sheet.pdf)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume I'm equally interested in the classification performance for each class in the dataset.\n",
    "\n",
    "# Create X data & y target dataframe's\n",
    "if 'ChildPoverty' in df:\n",
    "    y = df['ChildPoverty_Class'].values\n",
    "    del df['ChildPoverty_Class']\n",
    "    X = df.to_numpy()\n",
    "\n",
    "# Notice we only perform fit_transform on X\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Divide the data: 80% Training & 20% Testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"Training Set\", \"\\n   - Data Shape:\",X_train.shape,\"\\n   - Target Shape:\",y_train.shape)\n",
    "print(\"\\nTesting Set\",\"\\n   - Data Shape:\",X_test.shape ,\"\\n   - Target Shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Balancing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', RandomizedPCA(n_components=35,random_state=1)),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced', random_state=1, max_iter=500))])\n",
    "\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=ChildPovertyClass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)\n",
    "\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Acc: %.3f' % (k+1, score))\n",
    "\n",
    "print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split 80/20. Balance 80, argue whether 20 should be balanced\n",
    "- Explain reasoning for selecting this method for balancing the dataset. \n",
    "- Should balancing of the dataset be done for both the training & testing set? Explain.\n",
    "\n",
    "If you do it to the test set, you're misrepresenting the reality of the dataset.\n",
    "We do it on the training set so that our model doesn't overly classify just one child poverty rate.\n",
    "But on the testing set we don't want to introduce new data or massage the data on a test set.\n",
    "For the integrity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "------------------------\n",
    "\n",
    "## 2. Pre-Processing\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Quantifying Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Two-Layer Perceptron network example (Mini-Batching & Glorant) provided during lecture,\n",
    "# Let's quantify performance using accuracy. We don't normalize or one-hot encode the data just yet.\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted it only has internal classes to be used by classes that will subclass it\n",
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001,\n",
    "                 random_state=None, alpha=0.0,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, activation='sigmoid',\n",
    "                 obj_func='quadratic'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.activation = activation\n",
    "        self.obj_func = obj_func\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\" Encode labels into one-hot representation \"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        hidden_layers = self.n_hidden\n",
    "\n",
    "        if not hasattr(hidden_layers, \"__iter__\"): hidden_layers = [hidden_layers]\n",
    "\n",
    "        hidden_layers = list(hidden_layers)\n",
    "        hidden_layers += [self.n_output_]\n",
    "\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        weights = []\n",
    "        w1 = []\n",
    "        #self.n_features_ = self.n_features_\n",
    "        n_layers = len(hidden_layers)\n",
    "\n",
    "        for i, n_hidden in enumerate(hidden_layers):\n",
    "\n",
    "            if self.activation == 'sigmoid' or self.activation == 'linear':\n",
    "                w1_num_elems = (self.n_features_ + 1) * n_hidden\n",
    "                w1 = np.random.uniform(-1.0, 1.0, size=w1_num_elems)\n",
    "                w1 = w1.reshape(n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "            elif self.activation == 'relu' or self.activation == 'silu':\n",
    "                if i == n_layers - 1: init_bound = np.sqrt(0.5 / (n_hidden + self.n_features_ + 1))\n",
    "                else: init_bound = np.sqrt(6. / (n_hidden + self.n_features_ + 1))\n",
    "\n",
    "                w1 = np.random.uniform(-init_bound, init_bound, (n_hidden, self.n_features_ + 1))\n",
    "                w1[:, :1] = 0\n",
    "                \n",
    "            self.n_features_ = n_hidden\n",
    "            weights += [w1]\n",
    "            \n",
    "        return weights\n",
    "\n",
    "    # @staticmethod\n",
    "    def _activation(self, z):\n",
    "        if self.activation == 'linear': return z\n",
    "        if self.activation == 'sigmoid': return expit(z)\n",
    "        if self.activation == 'relu': return np.maximum(0, z.copy())\n",
    "        if self.activation == 'silu':\n",
    "            z = z.copy() * expit(z.copy())\n",
    "            return z\n",
    "        \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\" Add bias unit (column or row of 1s) to array at index 0 \"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        w = 0\n",
    "        for w_j in weights:\n",
    "            w += np.mean(w_j[:, 1:] ** 2)\n",
    "            \n",
    "        return (lambda_/2.0) * np.sqrt(w)\n",
    "        \n",
    "        #return (lambda_/2.0) * np.sqrt(np.mean(w1[:, 1:] ** 2) + np.mean(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self, a3, Y_enc, weights):\n",
    "        \"\"\" Get the objective function value \"\"\"\n",
    "        if self.obj_func == 'quadratic':\n",
    "            cost = np.mean((Y_enc - a3) ** 2)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            #cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3[-1])+(1-Y_enc)*np.log(1-a3[-1]))))\n",
    "            #cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3)+(1-Y_enc)*np.log(1-a3))))\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3)+(1-Y_enc)*np.log(1-a3))))\n",
    "        \n",
    "        L2_term = self._L2_reg(self.l2_C, weights)\n",
    "        return cost + L2_term\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \"\"\" Compute feedforward step \"\"\"\n",
    "        a_i = []\n",
    "        z_i = []\n",
    "        z = None\n",
    "        for idx, W in enumerate(weights):\n",
    "            if idx == 0:\n",
    "                a1 = self._add_bias_unit(X.T, how='row')\n",
    "            else:\n",
    "                a1 = self._activation(z)\n",
    "                a1 = self._add_bias_unit(a1, how='row')\n",
    "            z1 = W @ a1\n",
    "            a_i += [a1]\n",
    "            z_i += [z1]\n",
    "            z = z1\n",
    "        a_out = self._activation(z)\n",
    "        a_i += [a_out]\n",
    "        \n",
    "        return a_i, z_i\n",
    "\n",
    "    def _get_gradient(self, a, z, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        gradients = []\n",
    "        A_output = a[-1]\n",
    "        \n",
    "        if self.obj_func == 'quadratic':\n",
    "            V_last = -2 * (Y_enc - A_output) * A_output * (1-A_output)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            V_last = (A_output - Y_enc)\n",
    "        \n",
    "        i = 1\n",
    "        for A, W in zip(a[:-1][::-1], weights[::-1]):\n",
    "            if i == 1:\n",
    "                grad = V_last @ A.T         # no bias on final layer\n",
    "                V1 = (W.T @ V_last)         # back prop the sensitivity\n",
    "            else:\n",
    "                grad = V_last[1:,:] @ A.T   # dont back prop sensitivity of bias\n",
    "                if i != len(weights):\n",
    "                    V1 = (W.T @ V_last[1:, :])\n",
    "            \n",
    "            if len(weights)-1-i >= 0:\n",
    "\n",
    "                if self.activation == 'linear':\n",
    "                    V1 = V1\n",
    "\n",
    "                elif self.activation == 'sigmoid':\n",
    "                    V1 = A * (1 - A) * V1\n",
    "\n",
    "                elif self.activation == 'relu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    V1[Z1_with_bias<=0] = 0\n",
    "\n",
    "                elif self.activation == 'silu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    sig = expit(Z1_with_bias.copy())\n",
    "                    V1 = (A + (1-A) * sig * V1)\n",
    "                V_last = V1\n",
    "\n",
    "            # regularize weights that are not bias terms\n",
    "            grad[:, 1:] += W[:, 1:] * self.l2_C\n",
    "            gradients.insert(0, grad)\n",
    "            \n",
    "            i += 1\n",
    "        return gradients\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = y_enc.shape[0]\n",
    "        self.weights= self._initialize_weights()\n",
    "        \n",
    "        W_prev_list = []\n",
    "        for W in self.weights:\n",
    "            delta_W_prev = np.zeros(W.shape)\n",
    "            W_prev_list += [delta_W_prev]\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "                a, z = self._feedforward(X_data[idx], self.weights)\n",
    "\n",
    "                cost = self._cost(a[-1], Y_enc[:, idx], self.weights)\n",
    "                mini_cost.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradients = self._get_gradient(a, z, Y_enc[:,idx], self.weights)\n",
    "\n",
    "                delta_W_list = []\n",
    "                for gradient in gradients:\n",
    "                    delta_W = self.eta * gradient\n",
    "                    delta_W_list += [delta_W]\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def get_params(self,deep=True):\n",
    "        return {\"n_hidden\" : self.n_hidden,\n",
    "                \"activation\" : self.activation,\n",
    "                \"obj_func\" : self.obj_func}\n",
    "    \n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels \"\"\"\n",
    "        A3, _, = self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(A3[-1], axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n",
    "params = dict(n_hidden=(50,50,50,),\n",
    "              C=0.1,\n",
    "              epochs=300,\n",
    "              eta=0.001,\n",
    "              random_state=1,\n",
    "              alpha=0.001,\n",
    "              decrease_const=0.0001,\n",
    "              shuffle=True,\n",
    "              minibatches=50,\n",
    "              activation='relu',\n",
    "              obj_func='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "mlp = MultiLayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=10)\n",
    "yhat = mlp.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Normalizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now normalize the continuous numeric feature data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Normalizing & One Hot Encoding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now normalize the continuous numeric feature data AND one hot encode the categorical data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure that training converges by graphing the loss function vs. # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Comparing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of the three models I just trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any meaningful differences in performance?\n",
    "- In my own words,explain why these models have (or don't have) different performances.\n",
    "    - _Use one-hot encoding & normalization on the dataset for the remainder of this lab assignment._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Three-layer Support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sup for a third layer in the multi-layer perceptron.\n",
    "# Add sup for saving(& plotting after training is completed) the avg magnitude of the gradient for each layer, for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify the performance of the model & graph the magnitudes for each layer Vs. the # of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Four-layer Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous step, adding support for a fourth layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Five-layer Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the previous step, adding support for a fifth layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Adaptive Learning Technique w/ five-layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement an adaptive learning technique that was discussed in lecture & use it on the five layer network.\n",
    "# Don't use AdaM for the adaptive learning technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the performance of this model w/ & w/o the adaptive learning strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------\n",
    "\n",
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Adaptive Momentum\n",
    "- 5000 level student: I have free reign to provide additional analyses.\n",
    "- One idea: Implement adaptive momentum (AdaM) in the five layer neural network & quantify the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------------\n",
    "\n",
    "### Reference\n",
    "\n",
    "Kaggle. US Census Demographics. https://www.kaggle.com/muonneutrino/us-census-demographic-data/data (Accessed 10-24-2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
