{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Lab Assignment Four: Multi-Layer Perceptron\n",
    "\n",
    "#### Luis Garduno\n",
    "\n",
    "Dataset : https://www.kaggle.com/muonneutrino/us-census-demographic-data/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--------------------------------\n",
    "\n",
    "## 1. Load, Split, & Balance\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Loading Data & Adjustments  (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 72718 entries, 0 to 74000\n",
      "Data columns (total 36 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   TractId           72718 non-null  int64  \n",
      " 1   State             72718 non-null  int64  \n",
      " 2   TotalPop          72718 non-null  int64  \n",
      " 3   Men               72718 non-null  int64  \n",
      " 4   Women             72718 non-null  int64  \n",
      " 5   Hispanic          72718 non-null  float64\n",
      " 6   White             72718 non-null  float64\n",
      " 7   Black             72718 non-null  float64\n",
      " 8   Native            72718 non-null  float64\n",
      " 9   Asian             72718 non-null  float64\n",
      " 10  Pacific           72718 non-null  float64\n",
      " 11  VotingAgeCitizen  72718 non-null  int64  \n",
      " 12  Income            72718 non-null  float64\n",
      " 13  IncomeErr         72718 non-null  float64\n",
      " 14  IncomePerCap      72718 non-null  float64\n",
      " 15  IncomePerCapErr   72718 non-null  float64\n",
      " 16  Poverty           72718 non-null  float64\n",
      " 17  ChildPoverty      72718 non-null  float64\n",
      " 18  Professional      72718 non-null  float64\n",
      " 19  Service           72718 non-null  float64\n",
      " 20  Office            72718 non-null  float64\n",
      " 21  Construction      72718 non-null  float64\n",
      " 22  Production        72718 non-null  float64\n",
      " 23  Drive             72718 non-null  float64\n",
      " 24  Carpool           72718 non-null  float64\n",
      " 25  Transit           72718 non-null  float64\n",
      " 26  Walk              72718 non-null  float64\n",
      " 27  OtherTransp       72718 non-null  float64\n",
      " 28  WorkAtHome        72718 non-null  float64\n",
      " 29  MeanCommute       72718 non-null  float64\n",
      " 30  Employed          72718 non-null  int64  \n",
      " 31  PrivateWork       72718 non-null  float64\n",
      " 32  PublicWork        72718 non-null  float64\n",
      " 33  SelfEmployed      72718 non-null  float64\n",
      " 34  FamilyWork        72718 non-null  float64\n",
      " 35  Unemployment      72718 non-null  float64\n",
      "dtypes: float64(29), int64(7)\n",
      "memory usage: 20.5 MB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into memory & save it to a pandas data frame.\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/Datasets/acs2017_census_tract_data.csv\");\n",
    "\n",
    "# Zip State Name with ID number \n",
    "def_state = zip(df['State'].unique(), np.arange(52))\n",
    "\n",
    "# Remove any observations having missing data.\n",
    "df = df.dropna(axis=0, how='any')\n",
    "del df['County']\n",
    "\n",
    "# Encode State's as integers\n",
    "df['State'] = pd.factorize(df.State)[0]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Splitting the Dataset (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set \n",
      "   - Data Shape: (58174, 35) \n",
      "   - Target Shape: (58174,)\n",
      "\n",
      "Testing Set \n",
      "   - Data Shape: (14544, 35) \n",
      "   - Target Shape: (14544,)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "\n",
    "#df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.0,0.1,0.28,0.65,1.0],labels=['Okay','U.S. Poverty Rate','Very Poor','Extremely Poor']) # this creates a new variable\n",
    "#df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.0,0.1,0.28,0.65,1.0],labels=False) # this creates a new variable\n",
    "#df['ChildPoverty_Class'] = pd.qcut(df['ChildPoverty'],[0.0,0.17,0.28,0.65,1.0],labels=False) # this creates a new variable\n",
    "#df.ChildPoverty_Class.describe()\n",
    "#df = pd.cut(df['ChildPoverty'], 4)\n",
    "df['ChildPoverty'].quantile()\n",
    "\n",
    "# Create X data & y target dataframe's\n",
    "if 'ChildPoverty' in df:\n",
    "    y = df['ChildPoverty'].values\n",
    "    del df['ChildPoverty']\n",
    "    X = df.to_numpy()\n",
    "\n",
    "# Divide the data: 80% Training & 20% Testing.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Training Set\", \"\\n   - Data Shape:\",X_train.shape,\"\\n   - Target Shape:\",y_train.shape)\n",
    "print(\"\\nTesting Set\",\"\\n   - Data Shape:\",X_test.shape ,\"\\n   - Target Shape:\",y_test.shape)\n",
    "\n",
    "#df.ChildPoverty_Class.plot(kind='hist',alpha=0.5)   # Blue\n",
    "#plt.show()\n",
    "\n",
    "# Assume I'm equally interested in the classification performance for each class in the dataset.\n",
    "# Split the dataset into 80% for training and 20% for testing.\n",
    "\n",
    "# Option 1.\n",
    "# Figure out how I want to Quantize it  --> Quantize it --> Split it into 80/20\n",
    "\n",
    "# Option 2.\n",
    "# If introducing variables (Overlap Sampling) --> Split into 80/20 --> Introduce variables ONLY in dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29  48 211 ... 480 200 309]\n",
      "continuous\n",
      "multiclass\n",
      "multiclass\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-84b988ddcff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mpipe_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_scores_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot_filled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_scores\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_x_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'names' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "import numpy as np\n",
    "from sklearn                        import metrics, svm\n",
    "from sklearn.linear_model           import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "training_scores_encoded = lab_enc.fit_transform(y_train)\n",
    "print(training_scores_encoded)\n",
    "print(utils.multiclass.type_of_target(y_train))\n",
    "print(utils.multiclass.type_of_target(y_train.astype('int')))\n",
    "print(utils.multiclass.type_of_target(training_scores_encoded))\n",
    "\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                    ('pca', RandomizedPCA(n_components=35,random_state=1)),\n",
    "                    ('clf', LogisticRegression(class_weight='balanced', random_state=1, max_iter=10000))])\n",
    "\n",
    "pipe_lr.fit(X_train, training_scores_encoded)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=names))\n",
    "\n",
    "def plot_filled(train_scores,test_scores,train_x_axis, xlabel=''):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(train_x_axis, train_mean,\n",
    "             color='blue', marker='o',\n",
    "             markersize=5, label='training accuracy')\n",
    "\n",
    "    plt.fill_between(train_x_axis,\n",
    "                     train_mean + train_std,\n",
    "                     train_mean - train_std,\n",
    "                     alpha=0.15, color='blue')\n",
    "\n",
    "    plt.plot(train_x_axis, test_mean,\n",
    "             color='green', linestyle='--',\n",
    "             marker='s', markersize=5,\n",
    "             label='validation accuracy')\n",
    "\n",
    "    plt.fill_between(train_x_axis,\n",
    "                     test_mean + test_std,\n",
    "                     test_mean - test_std,\n",
    "                     alpha=0.15, color='green')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "%matplotlib inline \n",
    "\n",
    "train_sizes, train_scores, test_scores =learning_curve(estimator=pipe_lr,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                                                       cv=5,\n",
    "                                                       n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_filled(train_scores, test_scores, train_sizes, xlabel='Number of training samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources were used to gain an understanding of where the national poverty was during 2017.\n",
    "\n",
    "- <a href=\"https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-State-Fact-Sheet.pdf\" target=\"_top\">\n",
    "    <b>Child Poverty in America 2017: State Analysis</b>\n",
    "  </a>\n",
    "- <a href=\"https://www.childrensdefense.org/wp-content/uploads/2018/09/Child-Poverty-in-America-2017-National-Fact-Sheet.pdf\" target=\"_top\">\n",
    "    <b>Child Poverty in America 2017: National Analysis</b>\n",
    "  </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Balancing the Data(15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Balance the dataset so that about the same # of instances are within each class (divide into 4 classes by Child Poverty)\n",
    "# Once I divide this up by child poverty(4 classes), each class has about the same # of examples for each of those counties\n",
    "\n",
    "# Split 80/20. Balance 80, argue whether 20 should be balanced\n",
    "\n",
    "# Choose a method for balancing the dataset (Quantiles or Overlap Sampling(Only do it on the training set))\n",
    "# An option is to choose quantization thresholds for the \"ChildPoverty\" variable that equally divide the data into 4 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Explain reasoning for selecting this method for balancing the dataset. \n",
    "\n",
    "\n",
    "- Should balancing of the dataset be done for both the training & testing set? Explain.\n",
    "\n",
    "If you do it to the test set, you're misrepresenting the reality of the dataset.\n",
    "We do it on the training set so that our model doesn't overly classify just one child poverty rate.\n",
    "But on the testing set we don't want to introduce new data or massage the data on a test set.\n",
    "For the integrity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "------------------------\n",
    "\n",
    "## 2. Pre-Processing\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Quantifying Performance (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the example (try and use Glorant and mini-batching) two-layer perceptron network from the class example & quantify performance using accuracy.\n",
    "# Don't normalize or one-hot encode the data (not yet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Normalizing Data (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now normalize the continuous numeric feature data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Normalizing & One Hot Encoding the Data (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now normalize the continuous numeric feature data AND one hot encode the categorical data.\n",
    "# Use the example two-layer perceptron network from the class example & quantify performance using accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Be sure that training converges by graphing the loss function vs. # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Comparing the Models (40 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compare the performance of the three models I just trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any meaningful differences in performance?\n",
    "- In my own words,explain why these models have (or don't have) different performances.\n",
    "    - _Use one-hot encoding & normalization on the dataset for the remainder of this lab assignment._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "------------------\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Three-layer Support (50 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add sup for a third layer in the multi-layer perceptron.\n",
    "# Add sup for saving(& plotting after training is completed) the avg magnitude of the gradient for each layer, for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantify the performance of the model & graph the magnitudes for each layer Vs. the # of epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Four-layer Support (60 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat the previous step, adding support for a fourth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.3 Five-layer Support (70 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Repeat the previous step, adding support for a fifth layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.4 Adaptive Learning Technique w/ five-layer network (90 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement an adaptive learning technique that was discussed in lecture & use it on the five layer network.\n",
    "# Don't use AdaM for the adaptive learning technique. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Compare the performance of this model w/ & w/o the adaptive learning strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------------------\n",
    "\n",
    "## &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4. Adaptive Momentum (100 points)\n",
    "- 5000 level student: I have free reign to provide additional analyses.\n",
    "- One idea: Implement adaptive momentum (AdaM) in the five layer neural network & quantify the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----------------------\n",
    "\n",
    "### Reference\n",
    "\n",
    "Kaggle. US Census Demographics. https://www.kaggle.com/muonneutrino/us-census-demographic-data/data (Accessed 10-24-2020)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
