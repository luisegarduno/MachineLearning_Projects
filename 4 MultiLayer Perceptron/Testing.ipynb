{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "capital-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using the Two-Layer Perceptron network example (Mini-Batching & Glorant) provided during lecture,\n",
    "# Let's quantify performance using accuracy. We don't normalize or one-hot encode the data just yet.\n",
    "\n",
    "# start with a simple base classifier, which can't be fit or predicted it only has internal classes to be used by classes that will subclass it\n",
    "class MultiLayerPerceptron(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001,\n",
    "                 random_state=None, alpha=0.0,\n",
    "                 decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, activation='sigmoid',\n",
    "                 obj_func='quadratic'):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        self.activation = activation\n",
    "        self.obj_func = obj_func\n",
    "\n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\" Encode labels into one-hot representation \"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "        return onehot\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        hidden_layers = self.n_hidden\n",
    "\n",
    "        if not hasattr(hidden_layers, \"__iter__\"): hidden_layers = [hidden_layers]\n",
    "\n",
    "        hidden_layers = list(hidden_layers)\n",
    "        hidden_layers += [self.n_output_]\n",
    "\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        weights = []\n",
    "        #self.n_features_ = self.n_features_\n",
    "        n_layers = len(hidden_layers)\n",
    "\n",
    "        for i, n_hidden in enumerate(hidden_layers):\n",
    "\n",
    "            if self.activation == 'sigmoid' or self.activation == 'linear':\n",
    "                w1_num_elems = (self.n_features_ + 1) * n_hidden\n",
    "                w1 = np.random.uniform(-1.0, 1.0, size=w1_num_elems)\n",
    "                w1 = w1.reshape(n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "\n",
    "            elif self.activation == 'relu' or self.activation == 'silu':\n",
    "                if i == n_layers - 1: init_bound = np.sqrt(0.5 / (n_hidden + self.n_features_ + 1))\n",
    "                else: init_bound = np.sqrt(6. / (n_hidden + self.n_features_ + 1))\n",
    "\n",
    "                w1 = np.random.uniform(-init_bound, init_bound, (n_hidden, self.n_features_ + 1))\n",
    "                w1[:, :1] = 0\n",
    "                \n",
    "            self.n_features_ = n_hidden\n",
    "            weights += [w1]\n",
    "            \n",
    "        return weights\n",
    "\n",
    "    @staticmethod\n",
    "    def _activation(self, z):\n",
    "        if self.activation == 'linear': return z\n",
    "        if self.activation == 'sigmoid': return expit(z)\n",
    "        if self.activation == 'relu': return np.maximum(0, z.copy())\n",
    "        if self.activation == 'silu':\n",
    "            z = z.copy() * expit(z.copy())\n",
    "            return z\n",
    "        \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\" Add bias unit (column or row of 1s) to array at index 0 \"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, weights):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        w = 0\n",
    "        for w_j in weights:\n",
    "            w += np.mean(w_j[:, 1:] ** 2)\n",
    "            \n",
    "        return (lambda_/2.0) * np.sqrt(w)\n",
    "        \n",
    "        #return (lambda_/2.0) * np.sqrt(np.mean(w1[:, 1:] ** 2) + np.mean(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self, a3, Y_enc, weights):\n",
    "        \"\"\" Get the objective function value \"\"\"\n",
    "        if self.obj_func == 'quadratic':\n",
    "            cost = np.mean((Y_enc - a3) ** 2)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            cost = -np.mean(np.nan_to_num((Y_enc*np.log(a3)+(1-Y_enc)*np.log(1-a3))))\n",
    "        \n",
    "        L2_term = self._L2_reg(self.l2_C, weights)\n",
    "        return cost + L2_term\n",
    "        \n",
    "    def _feedforward(self, X, weights):\n",
    "        \"\"\" Compute feedforward step \"\"\"\n",
    "        a_i = []\n",
    "        z_i = []\n",
    "        z = None\n",
    "        for idx, W in enumerate(weights):\n",
    "            if idx == 0:\n",
    "                a1 = self._add_bias_unit(X.T, how='row')\n",
    "            else:\n",
    "                a1 = self._activation(z)\n",
    "                a1 = self._add_bias_unit(a1, how='row')\n",
    "            z1 = w @ a1\n",
    "            a_i += [a1]\n",
    "            z_i += [z1]\n",
    "            z = z1\n",
    "        a_out = self._sigmoid(z)\n",
    "        a_i += [a_out]\n",
    "        \n",
    "        return a_i, z_i\n",
    "\n",
    "    def _get_gradient(self, a, z, Y_enc, weights):\n",
    "        \"\"\" Compute gradient step using backpropagation. \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        gradients = []\n",
    "        A_output = a[-1]\n",
    "        \n",
    "        if self.obj_func == 'quadratic':\n",
    "            V_last = -2 * (Y_enc - A_output) * A_output * (1-A_output)\n",
    "        elif self.obj_func == 'cross_entropy':\n",
    "            V_last = (A_output - Y_enc)\n",
    "        \n",
    "        i = 1\n",
    "        for A, W in zip(a[:-1][::-1], weights[::-1]):\n",
    "            if i == 1:\n",
    "                grad = V_last @ A.T         # no bias on final layer\n",
    "                V1 = (W.T @ V_last)         # back prop the sensitivity\n",
    "            else:\n",
    "                grad = V_last[1:,:] @ A.T   # dont back prop sensitivity of bias\n",
    "                if i != len(weights):\n",
    "                    V1 = (W.T @ V_last[1:, :])\n",
    "            \n",
    "            if len(weights)-1-i >= 0:\n",
    "\n",
    "                if self.activation == 'linear':\n",
    "                    V1 = V1\n",
    "\n",
    "                elif self.activation == 'sigmoid':\n",
    "                    V1 = A * (1 - A) * V1\n",
    "\n",
    "                elif self.activation == 'relu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    V1[Z1_with_bias<=0] = 0\n",
    "\n",
    "                elif self.activation == 'silu':\n",
    "                    Z1_with_bias = self._add_bias_unit(z[len(weights)-1-i],how='row')\n",
    "                    V1 = (A + (1-A) * self._sigmoid(Z1_with_bias)) * V1\n",
    "                V_last = V1\n",
    "\n",
    "            # regularize weights that are not bias terms\n",
    "            grad[:, 1:] += W[:, 1:] * self.l2_C\n",
    "            gradients.insert(0, grad)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        return gradients\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = y_enc.shape[0]\n",
    "        self.weights= self._initialize_weights()\n",
    "        \n",
    "        W_prev_list = []\n",
    "        for W in self.weights_list:\n",
    "            delta_W_prev = np.zeros(W.shape)\n",
    "            W_prev_list += [delta_W_prev]\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc, y_data = X_data[idx_shuffle], y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a, z = self._feedforward(X_data[idx], self.weights)\n",
    "\n",
    "                cost = self._cost(a3,y_enc[:, idx],self.w1,self.w2)\n",
    "                mini_cost.append(cost)    # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a, z, y_enc=y_enc[:,idx], self.weights)\n",
    "\n",
    "                # momentum calculations\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels \"\"\"\n",
    "        A3, _, = self._feedforward(X, self.weights)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "level-election",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TLPMiniBatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0a4a8c06eb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTLPGlorot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTLPMiniBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# need to add to the original initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TLPMiniBatch' is not defined"
     ]
    }
   ],
   "source": [
    "class TLPGlorot(TLPMiniBatch):\n",
    "    def __init__(self, dropout=0.25, **kwds):\n",
    "        # need to add to the original initializer\n",
    "        self.dropout = dropout\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, print_progress=0, xy_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        x_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y)\n",
    "\n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = x_data.shape[1]\n",
    "        self.n_output_ = y_enc.shape[0]\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(x_data)))\n",
    "        if xy_test is not None:\n",
    "            X_test = xy_test[0].copy()\n",
    "            y_test = xy_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                x_data, y_enc, y_data = x_data[idx_shuffle], y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "\n",
    "            # adding dropout neurons\n",
    "            w1 = self.w1.copy()\n",
    "            w2 = self.w2.copy()\n",
    "\n",
    "            if self.dropout>0.0:\n",
    "\n",
    "                # randomly select half of the neurons\n",
    "                idx_dropout = np.random.permutation(w1.shape[0])\n",
    "                #idx_other_half = idx_dropout[:int(w1.shape[0]*self.dropout)]\n",
    "                idx_dropout = idx_dropout[int(w1.shape[0]*(1-self.dropout)):] #drop half\n",
    "\n",
    "                idx_dropout = np.sort(idx_dropout)\n",
    "                idx_w2_withbias = np.hstack(([0],(idx_dropout+1)))\n",
    "                w1 = w1[idx_dropout,:]# get rid of rows\n",
    "                w2 = w2[:,idx_w2_withbias]# get rid of extra columns\n",
    "                delta_w1_prev_dropout = delta_w1_prev[idx_dropout,:]\n",
    "                delta_w2_prev_dropout = delta_w2_prev[:,idx_w2_withbias]\n",
    "            else:\n",
    "                delta_w1_prev_dropout = delta_w1_prev\n",
    "                delta_w2_prev_dropout = delta_w2_prev\n",
    "\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z1, a2, z2, a3 = self._feedforward(x_data[idx], w1, w2)\n",
    "\n",
    "                cost = self._cost(a3,y_enc[:, idx],w1,w2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z1=z1, z2=z2, y_enc=y_enc[:, idx], w1=w1,w2=w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                w1 -= (delta_w1 + (self.alpha * delta_w1_prev_dropout))\n",
    "                w2 -= (delta_w2 + (self.alpha * delta_w2_prev_dropout))\n",
    "                delta_w1_prev_dropout, delta_w2_prev_dropout = delta_w1, delta_w2\n",
    "\n",
    "            if self.dropout>0.0:\n",
    "                # now append the learned weights back into the original matrices\n",
    "                self.w1[idx_dropout,:] = w1\n",
    "                self.w2[:,idx_w2_withbias] = w2\n",
    "                delta_w1_prev[idx_dropout,:] = delta_w1_prev_dropout\n",
    "                delta_w2_prev[:,idx_w2_withbias] = delta_w2_prev_dropout\n",
    "            else:\n",
    "                # don't eliminate any neurons\n",
    "                self.w1 = w1\n",
    "                self.w2 = w2\n",
    "                delta_w1_prev = delta_w1_prev_dropout\n",
    "                delta_w2_prev = delta_w2_prev_dropout\n",
    "\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(x_data)))\n",
    "            self.cost_.append(mini_cost) # only uses dropped samples, so more noise\n",
    "            if xy_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contemporary-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import plotly\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ds = load_iris()\n",
    "X = ds.data\n",
    "y = ds.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit(X))\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "international-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(n_hidden=(50,50,50,),\n",
    "              C=0.1,\n",
    "              epochs=300,\n",
    "              eta=0.001,\n",
    "              random_state=1,\n",
    "              alpha=0.001,\n",
    "              decrease_const=0.0001,\n",
    "              shuffle=True,\n",
    "              minibatches=50,\n",
    "              activation='silu',\n",
    "              obj_func='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "greater-hughes",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiLayerPerceptron' object has no attribute 'w1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-1261a41e43c2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, print_progress)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mdelta_w1_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mdelta_w2_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiLayerPerceptron' object has no attribute 'w1'"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "mlp = MultiLayerPerceptron(**params)\n",
    "mlp.fit(X_train, y_train, print_progress=10)\n",
    "yhat = mlp.predict(X_test)\n",
    "print('Test acc:',accuracy_score(y_test,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-richmond",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
