{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab Assignment Seven: RNNs\n",
    "\n",
    "### Luis Garduno\n",
    "\n",
    "Dataset : [Sentiment140 Kaggle Dataset](https://www.kaggle.com/kazanova/sentiment140) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "----------------------------\n",
    "\n",
    "## 1. Preparation\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Total Tweets: 1600000 \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   query     1600000 non-null  object\n",
      " 4   user      1600000 non-null  object\n",
      " 5   tweet     1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tkinter import Tcl\n",
    "from skimage.io import imshow\n",
    "from sklearn import metrics as mt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Load in Sentiment140 Dataset in pandas array\n",
    "df = pd.read_csv('../data/Sentiment140/training.1600000.processed.noemoticon.csv',\n",
    "                 names=['polarity', 'id', 'date', 'query', 'user', 'tweet'],\n",
    "                 encoding='ISO-8859-1')\n",
    "\n",
    "print(\"Number of Total Tweets:\", len(df), \"\\n\")\n",
    "\n",
    "# Check if our dataset is missing any columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Field | Description | Type |\n",
    "| ----- | ----------- | ---- |\n",
    "| **Polarity** | &nbsp;&nbsp;&nbsp; - The target/polarity of the tweet <br /> &nbsp;&nbsp; [0] Negative; [2] Neutral; [4] Positive; | Discrete |\n",
    "| **id**    | &nbsp;&nbsp;&nbsp; - The id of the tweet    | Discrete |\n",
    "| **date**  | &nbsp;&nbsp;&nbsp; - The date of the tweet  | Interval |\n",
    "| **query** | &nbsp;&nbsp;&nbsp; - The query | Nominal |\n",
    "| **user**  | &nbsp;&nbsp;&nbsp; - The user that tweeted  | nominal  |\n",
    "| **tweet**  | &nbsp;&nbsp;&nbsp; - The text of the tweet  | nominal  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity          2\n",
       "id          1598315\n",
       "date         774363\n",
       "query             1\n",
       "user         659775\n",
       "tweet       1581466\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import missingno as mn\n",
    "\n",
    "# Check for any missing/null values within dataset\n",
    "mn.matrix(df)\n",
    "\n",
    "# Find unique values within each field, before dropping columns.\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After outputting all of the unique values, we see that there is only 1 unique value\n",
    "within the query field, meaning that it will not make an impact when further doing\n",
    "our analysis because all the values are the same. Other fields that can be dropped\n",
    "are the user tweet id, username, iand date because these these do not impact whether a\n",
    "user's tweet is negative or positive.\n",
    "\n",
    "Addtionally we're able to see that there's 2 unique values within the polarity field,\n",
    "it is probably best to find out what these two values are and change them so that 0\n",
    "equals negative and 1 equals positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the fields we don't need\n",
    "df.drop(['id', 'date', 'query','user'], axis=1, inplace=True)\n",
    "\n",
    "# Find the 2 unique polarity fields\n",
    "df['polarity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Tweets')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEWCAYAAABbgYH9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApEklEQVR4nO3dfZyVdZ3/8dc7MMM7BEV/CNhMQjfqlsmEVFva0gJtN7ClNVY6FUW5bPdbaWthuq76q3Rzd3Vj00DXRKIbyTKdcL2pCBjvQryJySEhCNEhRVMT+uwf1/fIxfHMmTPMXGdg5v18PM7jnPO5ru/3+p7DYT7X9f1e1/dSRGBmZtbXXtDfDTAzs4HJCcbMzArhBGNmZoVwgjEzs0I4wZiZWSGcYMzMrBBOMFaRpP+S9KU+qutwSU9IGpLe3yzpw31Rd6rvekktfVVfrt4++w56sM1efTeSVks6oe9a1PttVvtM5b8NG1iG9ncDrP4krQUOBbYB24F7gSuAeRHxF4CI+FgP6vpwRPysq3Ui4iFgv961+rntnQWMj4j35+p/S1/UXa7W76ASSTcDk8m+46eBW4E5EbGxb1pXWUQclWvDWZR9V7vbNnf1tyHpfcA309shwN7An3L19snvrYttnwD8T0SMLWobA4WPYAavt0fE/sCLgfOBLwCX9fVGJA3mnZh/TH/oXgocCFxU1IYG2/ccEVdFxH7p+30LsKH0vsjkYj3jBDPIRcRjEbEEeA/QIuloAEnzJf1Len2wpOsk/VFSp6TbJL1A0pXA4cCPUjfH5yU1SApJsyQ9BNyUi+X/CB4haYWkxyRdK2lk2tYJktbn2yhpraQ3S5oOfBF4T9re3Wn5c10wqV1nSvqdpIclXSFpeFpWakeLpIckPSLpn7v6bsq+gxMkrZf02VTvRkkfrPE77gS+B5S+29dJWpk++0pJr+ti+0dIuknSo6mtV0k6sOx7+YKkXwNPShpa7buSdJKk28u28VlJP6yw7TdJWpV7/zNJK3Lvfy5pZq4dXf77JC+W9AtJWyXdKOngVHan30b6tzyn0rq1kPRBST/KvW+XtCj3fp2kY9Lrl0tqTb/pByS9O7fe3pK+ln4nm5R1lw6TtC9wPXBY+oxPSDpM0iRJbZIeT+tfWGubBzInGAMgIlYA64E3VFj82bRsFFnX2hezInEK8BDZ0dB+EfH/c2WOB14BTOtik6cCHwIOI+tGuriGNv4U+FfgmrS9V1VY7QPp8SbgJWTdL/9Rts5fAy8DpgBflvSK7rad/D9gODAGmAX8p6QR3RVKfyDfBdyZEumPyT7vQcCFwI8lHVSpKHAe2Xf0CmAccFbZOicDbwUOjIhtpWAX39USoLHs874fuLLCtpcB45XtXAwlS45jJe0vaRgwEbgtX6Cbf5/3Ah8EDgFeCPxThW3uyrrlbgHekHY0RgN7Aa8HkFT6Pfw6JYpW4DtpOycDl0gqdfddQHbkeQwwnuzf/MsR8STPP2LaAHwD+EZEHAAcATyX1AYzJxjL2wCMrBB/FhgNvDgino2I26L7SezOiognI+KpLpZfGRH3pP+wXwLerb4Z6H0fcGFEPBgRTwBnAM3a+ejpKxHxVETcDdwNVEpUlTwLnJ2+g58AT5Alqq5cLOmPaRsbgc+QJYM1EXFlRGyLiKuB+4G3lxeOiPaIaI2IZyJiM1kyOr58GxGxrsr3nK/vGeAasqRC+mPaAFxXYd2ngTbgjUAT8Gvg52R/rCenz/Bod9vM+XZE/Ca1cxHZH+6+WLe83Q8CW1OZ44EbgN9Lenl6f1saZ3wbsDYivp3+He4gO8o8UZKAjwCfjojOiNhKljibq2z6WVJCjognIuJXtbZ5IBtU/bbWrTFAZ4X4V8n2nG/M/u8xLyLO76audT1Y/juyPc2au0KqOCzVl697KNmRV8kfcq//RO2DzI/mjxJqKPuJiPhWPiCpvH2lNo4pLyzpELIjnTcA+5PtEG4pW62777ncAuBqSWcCpwCLUuKp5BbgBLKj11vSto8Hnknve6In3/mu/vuUlNo9Pr3+I1m7X8uOdr8YOC7tAJQMJTuaGwXsA9yefu+QHU1W2wGaBZwN3C+pg2wn5nmJe7DxEYwBIOk1ZH/kfl6+LCK2RsRnI+IlZHvan5E0pbS4iyq7O8IZl3t9ONke4CPAk2T/uUvtGkL2H77WejeQ/fHI170N2NRNuXopbx9kbfx9hXXPI/u8r0xdL+8n+0OXV+37eN6ytGf9Z7Kk9V4qd4+VlP5QvzG9voXsD/XxdJ1gdofp2UvtfgNdt3sdcEtEHJh77BcRp5H9Dp8CjsotG547eaDS97omIk4m6267AFicuuEGNSeYQU7SAZLeBiwkO/VyVYV13iZpfOo6eJzs1ObtafEmsrGOnnq/pCMl7UO257c4IrYDvwFeJOmtkvYCziQ7BbVkE9Agqavf7tXApyU1StqPHWMC27pYv95+ArxU0nvToPx7gCOp0E1FdtTyBPBHSWOAz/VwW119V1eQjUtti4jn7VDk/JKsC3ASsCIiVpP2/MlOu+7JNuvpFrIxuGERsZ5srGg62ZjXnWmd68j+HU6RtFd6vEbSK1IX2n8DF6WjSCSNkVQaT9wEHKR08kha/n5Jo1LZP6Zw6f/IoOUEM3j9SNJWsj25fybr3+/qrKgJwM/I/tgtAy6JiJvTsvOAM5WdYdaTwdgrgflk3SEvAj4B2VltwD8A3yLbq3+SrIum5Lvp+VFJd1So9/JU961AB9k1KB/vQbsKlcYt3kZ24sSjwOeBt0XEIxVW/wpwLPAY2YkB3+/h5rr6rq4kG7SvdvRCGh+7A1gdEX9O4WXA7yLi4R5us24i4jdkv9Xb0vvHgQeBX6SdGNK4ylSycZUNZL/DC9ixM/MFoB34laTHyX7/L0tl7yfbkXkw/e4PI0tgqyU9QTbg35zGsQY1dT9Wa2YDSToL7GHg2IhY09/tsYHLRzBmg89pwEonFyuazyIzG0SUTe0jYGb/tsQGA3eRmZlZIdxFZmZmhXAXWXLwwQdHQ0NDfzfDzGyPcvvttz8SEaMqLXOCSRoaGmhra+vvZpiZ7VEklc9M8Rx3kZmZWSGcYMzMrBBOMGZmVggnGDMzK4QTjJmZFcIJxszMClFogpH0aUmrJd0j6WpJL5I0Mt0He016HpFb/4x0D+0HclNjI2mipFVp2cVp2vjSfbOvSfHlkhpyZVrSNtZIainyc5qZ2fMVlmDS/Ss+ATRFxNFkd4NrBk4HlkbEBGBpeo+kI9Pyo8imvr4kdwvdS4HZZNPGT0jLIbuL3JaIGA9cRDbdNum+53PJ7lsxCZhby73Tzcys7xTdRTYUGJbuh74P2X0XZpDdtpX0PDO9ngEsTPcf7yC7F8MkSaOBAyJiWboP/BVlZUp1LQampKObaUBrup/2FqCVHUnJzMzqoLAr+SPi95K+BjxEdvvRGyPiRkmHRsTGtM7G0h3jyG7X+6tcFetT7Fl2vuFUKV4qsy7VtU3SY2R3rXsuXqHMcyTNJjsy4vDDD+/Fp82sbWzsdR028DR0dPR3EwBobFzb302w3VBHR0NhdRfZRTaC7AijETgM2FfS+6sVqRCLKvFdLbMjEDEvIpoiomnUqIpT6ZiZ2S4qsovszUBHRGyOiGfJbvf6OmBT6vYiPZduvboeGJcrP5asS219el0e36lM6oYbDnRWqcvMzOqkyATzEDBZ0j5pXGQKcB+wBCid1dUCXJteLwGa05lhjWSD+StSd9pWSZNTPaeWlSnVdSJwUxqnuQGYKmlEOpKammJmZlYnRY7BLJe0GLgD2AbcCcwD9gMWSZpFloROSuuvlrQIuDetPycitqfqTgPmA8OA69MD4DLgSkntZEcuzamuTknnACvTemdHRGdRn9XMzJ7Pd7RMmpqaorfT9XuQ3yrxIL/tzno7yC/p9ohoqrTMV/KbmVkhnGDMzKwQTjBmZlYIJxgzMyuEE4yZmRXCCcbMzArhBGNmZoVwgjEzs0I4wZiZWSGcYMzMrBBOMGZmVggnGDMzK4QTjJmZFcIJxszMCuEEY2ZmhXCCMTOzQhSWYCS9TNJducfjkj4laaSkVklr0vOIXJkzJLVLekDStFx8oqRVadnF6dbJpNsrX5PiyyU15Mq0pG2skdSCmZnVVWEJJiIeiIhjIuIYYCLwJ+AHwOnA0oiYACxN75F0JNktj48CpgOXSBqSqrsUmA1MSI/pKT4L2BIR44GLgAtSXSOBucBxwCRgbj6RmZlZ8erVRTYF+G1E/A6YASxI8QXAzPR6BrAwIp6JiA6gHZgkaTRwQEQsi+z+zleUlSnVtRiYko5upgGtEdEZEVuAVnYkJTMzq4N6JZhm4Or0+tCI2AiQng9J8THAulyZ9Sk2Jr0uj+9UJiK2AY8BB1Wpy8zM6qTwBCPphcA7gO92t2qFWFSJ72qZfNtmS2qT1LZ58+ZummdmZj1RjyOYtwB3RMSm9H5T6vYiPT+c4uuBcblyY4ENKT62QnynMpKGAsOBzip17SQi5kVEU0Q0jRo1apc/oJmZPV89EszJ7OgeA1gClM7qagGuzcWb05lhjWSD+StSN9pWSZPT+MqpZWVKdZ0I3JTGaW4ApkoakQb3p6aYmZnVydAiK5e0D/C3wEdz4fOBRZJmAQ8BJwFExGpJi4B7gW3AnIjYnsqcBswHhgHXpwfAZcCVktrJjlyaU12dks4BVqb1zo6IzkI+pJmZVVRogomIP5ENuudjj5KdVVZp/XOBcyvE24CjK8SfJiWoCssuBy7veavNzKwv+Ep+MzMrhBOMmZkVwgnGzMwK4QRjZmaFcIIxM7NCOMGYmVkhnGDMzKwQTjBmZlYIJxgzMyuEE4yZmRXCCcbMzArhBGNmZoVwgjEzs0I4wZiZWSGcYMzMrBBOMGZmVggnGDMzK0ShCUbSgZIWS7pf0n2SXitppKRWSWvS84jc+mdIapf0gKRpufhESavSsoslKcX3lnRNii+X1JAr05K2sUZSS5Gf08zMnq/oI5hvAD+NiJcDrwLuA04HlkbEBGBpeo+kI4Fm4ChgOnCJpCGpnkuB2cCE9Jie4rOALRExHrgIuCDVNRKYCxwHTALm5hOZmZkVr7AEI+kA4I3AZQAR8eeI+CMwA1iQVlsAzEyvZwALI+KZiOgA2oFJkkYDB0TEsogI4IqyMqW6FgNT0tHNNKA1IjojYgvQyo6kZGZmdVDkEcxLgM3AtyXdKelbkvYFDo2IjQDp+ZC0/hhgXa78+hQbk16Xx3cqExHbgMeAg6rUtRNJsyW1SWrbvHlzbz6rmZmVKTLBDAWOBS6NiFcDT5K6w7qgCrGoEt/VMjsCEfMioikimkaNGlWlaWZm1lNFJpj1wPqIWJ7eLyZLOJtStxfp+eHc+uNy5ccCG1J8bIX4TmUkDQWGA51V6jIzszopLMFExB+AdZJelkJTgHuBJUDprK4W4Nr0egnQnM4MayQbzF+RutG2SpqcxldOLStTqutE4KY0TnMDMFXSiDS4PzXFzMysTob2ZOX0x3pcRPy6xiIfB66S9ELgQeCDZEltkaRZwEPASQARsVrSIrIktA2YExHbUz2nAfOBYcD16QHZCQRXSmonO3JpTnV1SjoHWJnWOzsiOnvyWc3MrHe6TTCSbgbekda9C9gs6ZaI+Ex3ZSPiLqCpwqIpXax/LnBuhXgbcHSF+NOkBFVh2eXA5d210czMilFLF9nwiHgceCfw7YiYCLy52GaZmdmerpYEMzQNxr8buK7g9piZ2QBRS4L5CtkAeXtErJT0EmBNsc0yM7M9XS2D/Bsj4pWlNxHxoKQLC2yTmZkNALUcwfx7jTEzM7PndHkEI+m1wOuAUZLyZ4wdAAypXMrMzCxTrYvshcB+aZ39c/HHyS5qNDMz61KXCSYibgFukTQ/In4nad+IeLKObTMzsz1YLWMwh0m6l+xeLkh6laRLim2WmZnt6WpJMP9Gdn+VRwEi4m6y+7yYmZl1qabJLiNiXVloe8UVzczMklqug1kn6XVApEkrP0HqLjMzM+tKLUcwHwPmsOPOksek92ZmZl3q9ggmIh4B3leHtpiZ2QDS7RGMpJdKWirpnvT+lZLOLL5pZma2J6uli+y/gTOAZwHSzcaai2yUmZnt+WpJMPtExIqy2LYiGmNmZgNHLQnmEUlHAAEg6URgYy2VS1oraZWkuyS1pdhISa2S1qTnEbn1z5DULukBSdNy8YmpnnZJF0tSiu8t6ZoUXy6pIVemJW1jjaSWWtprZmZ9p5YEMwf4JvBySb8HPkV2Zlmt3hQRx0RE6dbJpwNLI2ICsDS9R9KRZF1vRwHTgUsklSbVvBSYDUxIj+kpPgvYEhHjgYuAC1JdI4G5wHHAJGBuPpGZmVnxuk0wEfFgRLwZGAW8PCL+OiJ+14ttzgAWpNcLgJm5+MKIeCYiOoB2YFK6m+YBEbEsIgK4oqxMqa7FwJR0dDMNaI2IzojYArSyIymZmVkd1HIW2W8lXQWcAozrYf0B3CjpdkmzU+zQiNgIkJ4PSfExQH7GgPUpVrr+pjy+U5mI2AY8BhxUpa7yzzZbUpukts2bN/fwo5mZWTW1dJEdSdZFdhDwNUkPSvpBjfW/PiKOBd4CzJFUbQ4zVYhFlfiultkRiJgXEU0R0TRq1KgqTTMzs56qJcFsJztFeTvwF2AT8HAtlUfEhvT8MPADsvGQTanbi/Rcqms9Ox8hjQU2pPjYCvGdykgaCgwHOqvUZWZmdVJLgnmcbEblDqAlIl4bER/trpCkfSXtX3oNTAXuAZYApbO6WoBr0+slQHM6M6yRbDB/RepG2yppchpfObWsTKmuE4Gb0jjNDcBUSSPS4P7UFDMzszqpZbLLk4G/Bv4B+LCkXwK3RsTSbsodCvwgnVE8FPhORPxU0kpgkaRZwEPASQARsVrSIuBesuts5kREadbm04D5wDDg+vQAuAy4UlI72ZFLc6qrU9I5wMq03tkR0VnDZzUzsz6ibIe/hhWll5ONpXwKOCQihhXYrrpramqKtra2XtWxtrGxj1pjA0lDR0d/NwGAxsa1/d0E2w11dDT0qryk23OXoeykyy4ySTem5+9J+i3wDWBfsi4qX1NiZmZVVesiOzg9nw/ckeuuMjMz61a1BHOgpHem1+PSWMpzIuL7hbXKzMz2eNUSzHDgbXR9TYkTjJmZdalagvldRHyobi0xM7MBpdp1MJWOXMzMzGpSLcGcUrdWmJnZgNNlgomIe+rZEDMzG1hqmSrGzMysx6pdaLk0PV9Qv+aYmdlAUe0sstGSjgfeIWkhZYP+EXFHoS0zM7M9WrUE82Wy2xmPBS4sWxbA3xTVKDMz2/N1mWAiYjGwWNKXIuKcOrbJzMwGgG6n64+IcyS9AyjdjfLmiLiu2GaZmdmertuzyCSdB3yS7D4t9wKfTDEzM7Mu1XLDsbcCx0TEXwAkLQDuBM4osmFmZrZnq/U6mANzr4cX0A4zMxtgakkw5wF3Spqfjl5uB/611g1IGiLpTknXpfcjJbVKWpOeR+TWPUNSu6QHJE3LxSdKWpWWXax07wBJe0u6JsWXS2rIlWlJ21gjqaXW9pqZWd/oNsFExNXAZLLp+b8PvDYiFvZgG58E7su9Px1YGhETgKXpPZKOBJqBo4DpwCWShqQylwKzgQnpMT3FZwFbImI8cBFwQaprJDAXOA6YBMzNJzIzMyteTV1kEbExIpZExLUR8YdaK5c0lmwM51u58AxgQXq9AJiZiy+MiGciogNoByZJGg0cEBHLIiKAK8rKlOpaDExJRzfTgNaI6IyILUArO5KSmZnVQdFzkf0b8HngL7nYoRGxEbLEBRyS4mOAdbn11qfYmPS6PL5TmYjYBjwGHFSlrp1Imi2pTVLb5s2bd+HjmZlZVwpLMJLeBjwcEbfXWqRCLKrEd7XMjkDEvIhoioimUaNG1dhMMzOrRdUEI+kFknZ12v7Xk81jthZYCPyNpP8BNqVuL9Lzw2n99cC4XPmxwIYUH1shvlMZSUPJznDrrFKXmZnVSdUEk659uVvS4T2tOCLOiIixEdFANnh/U0S8H1gClM7qagGuTa+XAM3pzLBGssH8FakbbaukyWl85dSyMqW6TkzbCOAGYKqkEWlwf2qKmZlZndRyoeVoYLWkFcCTpWBEvGMXt3k+sEjSLOAh4KRU32pJi8hmC9gGzImI7anMacB8YBhwfXoAXAZcKamd7MilOdXVKekcYGVa7+yI6NzF9pqZ2S5QtsNfZYVsyv7niYhbCmlRP2lqaoq2trZe1bG2sbGPWmMDSUNHR383AYDGxrX93QTbDXV0NPSqvKTbI6Kp0rJaJru8RdKLgQkR8TNJ+wBDuitnZmaDWy2TXX6E7BqTb6bQGOCHBbbJzMwGgFpOU55DdkbY4wARsYYd166YmZlVVEuCeSYi/lx6k04Hrj5wY2Zmg14tCeYWSV8Ehkn6W+C7wI+KbZaZme3pakkwpwObgVXAR4GfAGcW2SgzM9vz1XIW2V/SNP3LybrGHojuzm02M7NBr9sEI+mtwH8BvyWb46tR0kcj4vrqJc3MbDCr5Ur+rwNvioh2AElHAD9mx9X0ZmZmz1PLGMzDpeSSPMiOCSrNzMwq6vIIRtI708vVkn4CLCIbgzmJHXN8mZmZVVSti+ztudebgNKcZJsB337YzMyq6jLBRMQH69kQMzMbWGo5i6wR+DjQkF+/F9P1m5nZIFDLWWQ/JLvvyo+AvxTaGjMzGzBqSTBPR8TFhbfEzMwGlFoSzDckzQVuBJ4pBSPijsJaZWZme7xaroP5K+AjZLc6/np6fK27QpJeJGmFpLslrZb0lRQfKalV0pr0PCJX5gxJ7ZIekDQtF58oaVVadrEkpfjekq5J8eWSGnJlWtI21khqqfH7MDOzPlLLEczfAy/JT9lfo2eAv4mIJyTtBfxc0vXAO4GlEXG+pNPJJtP8gqQjgWbgKOAw4GeSXhoR24FLgdnAr8gm25xONpPALGBLRIyX1AxcALxH0khgLtBEdu3O7ZKWRMSWHn4GMzPbRbUcwdwNHNjTiiPzRHq7V3oEMANYkOILgJnp9QxgYUQ8ExEdQDswSdJo4ICIWJYm2byirEyprsXAlHR0Mw1ojYjOlFRayZKSmZnVSS1HMIcC90tayc5jMN2epixpCHA7MB74z4hYLunQiNiY6tgoqXR3zDFkRygl61Ps2fS6PF4qsy7VtU3SY8BB+XiFMvn2zSY7MuLwww/v7uOYmVkP1JJg5u5q5al76xhJBwI/kHR0ldVVqYoq8V0tk2/fPGAeQFNTk29BYGbWh2q5H8wtvd1IRPxR0s1k3VSbJI1ORy+j2TFx5npgXK7YWGBDio+tEM+XWZ9u5Twc6EzxE8rK3Nzbz2FmZrXrdgxG0lZJj6fH05K2S3q8hnKj0pELkoYBbwbuB5YApbO6WoBr0+slQHM6M6wRmACsSN1pWyVNTuMrp5aVKdV1InBTGqe5AZgqaUQ6S21qipmZWZ3UcgSzf/69pJnApBrqHg0sSOMwLwAWRcR1kpYBiyTNAh4im52ZiFgtaRFwL7ANmJO62ABOA+YDw8jOHivdi+Yy4EpJ7WRHLs2prk5J57Bj1uezI6KzhjabmVkf0a7c/VjSryJicgHt6TdNTU3R1tbWqzrWNjb2UWtsIGno6OjvJgDQ2Li2v5tgu6GOjoZelZd0e0Q0VVpWy2SX78y9fQE7ri0xMzPrUi1nkeXvC7MNWEt2/YmZmVmXahmD8X1hzMysx6rdMvnLVcpFRJxTQHvMzGyAqHYE82SF2L5k838dBDjBmJlZl6rdMvnrpdeS9gc+CXwQWEg2o7KZmVmXqo7BpFmJPwO8j2xSyWM9I7GZmdWi2hjMV8mm1p8H/FVuZmQzM7NuVZsq5rNk92U5E9iQmy5may1TxZiZ2eBWbQymlnvFmJmZVeQkYmZmhXCCMTOzQjjBmJlZIZxgzMysEE4wZmZWCCcYMzMrhBOMmZkVorAEI2mcpP+VdJ+k1ZI+meIjJbVKWpOeR+TKnCGpXdIDkqbl4hMlrUrLLpakFN9b0jUpvlxSQ65MS9rGGkktRX1OMzOrrMgjmG3AZyPiFcBkYI6kI4HTgaURMQFYmt6TljUDRwHTgUskDUl1XQrMBiakx/QUnwVsiYjxwEXABamukcBc4DhgEjA3n8jMzKx4hSWYiNgYEXek11uB+4AxZHfDXJBWWwDMTK9nAAsj4pmI6ADagUmSRgMHRMSyiAjgirIypboWA1PS0c00oDUiOtPknK3sSEpmZlYHdRmDSV1XrwaWA4dGxEbIkhBwSFptDLAuV2x9io1Jr8vjO5WJiG3AY2T3qumqrvJ2zZbUJqlt8+bNvfiEZmZWrvAEI2k/4HvApyKi2iSZqhCLKvFdLbMjEDEvIpoiomnUqFFVmmZmZj1VaIKRtBdZcrkqIr6fwptStxfp+eEUXw+MyxUfC2xI8bEV4juVkTQUGA50VqnLzMzqpMizyARcBtwXERfmFi0BSmd1tQDX5uLN6cywRrLB/BWpG22rpMmpzlPLypTqOhG4KY3T3ABMlTQiDe5PTTEzM6uTqne07KXXA6cAqyTdlWJfBM4HFkmaBTwEnAQQEaslLQLuJTsDbU5EbE/lTgPmA8OA69MDsgR2paR2siOX5lRXp6RzgJVpvbMjorOgz2lmZhUUlmAi4udUHgsBmNJFmXOBcyvE24CjK8SfJiWoCssuBy6vtb1mZta3fCW/mZkVwgnGzMwK4QRjZmaFcIIxM7NCOMGYmVkhnGDMzKwQTjBmZlYIJxgzMyuEE4yZmRXCCcbMzArhBGNmZoVwgjEzs0I4wZiZWSGcYMzMrBBOMGZmVggnGDMzK4QTjJmZFaKwBCPpckkPS7onFxspqVXSmvQ8IrfsDEntkh6QNC0XnyhpVVp2sSSl+N6Srknx5ZIacmVa0jbWSGop6jOamVnXijyCmQ9ML4udDiyNiAnA0vQeSUcCzcBRqcwlkoakMpcCs4EJ6VGqcxawJSLGAxcBF6S6RgJzgeOAScDcfCIzM7P6KCzBRMStQGdZeAawIL1eAMzMxRdGxDMR0QG0A5MkjQYOiIhlERHAFWVlSnUtBqako5tpQGtEdEbEFqCV5yc6MzMrWL3HYA6NiI0A6fmQFB8DrMuttz7FxqTX5fGdykTENuAx4KAqdT2PpNmS2iS1bd68uRcfy8zMyu0ug/yqEIsq8V0ts3MwYl5ENEVE06hRo2pqqJmZ1abeCWZT6vYiPT+c4uuBcbn1xgIbUnxshfhOZSQNBYaTdcl1VZeZmdVRvRPMEqB0VlcLcG0u3pzODGskG8xfkbrRtkqanMZXTi0rU6rrROCmNE5zAzBV0og0uD81xczMrI6GFlWxpKuBE4CDJa0nO7PrfGCRpFnAQ8BJABGxWtIi4F5gGzAnIranqk4jOyNtGHB9egBcBlwpqZ3syKU51dUp6RxgZVrv7IgoP9nAzMwKVliCiYiTu1g0pYv1zwXOrRBvA46uEH+alKAqLLscuLzmxpqZWZ/bXQb5zcxsgHGCMTOzQjjBmJlZIZxgzMysEE4wZmZWCCcYMzMrhBOMmZkVwgnGzMwK4QRjZmaFcIIxM7NCOMGYmVkhnGDMzKwQTjBmZlYIJxgzMyuEE4yZmRXCCcbMzArhBGNmZoUY0AlG0nRJD0hql3R6f7fHzGwwGbAJRtIQ4D+BtwBHAidLOrJ/W2VmNngM2AQDTALaI+LBiPgzsBCY0c9tMjMbNIb2dwMKNAZYl3u/Hjguv4Kk2cDs9PYJSQ/UqW2DwcHAI/3diN2C1N8tsOfz7zPpg5/ni7taMJATTKWvLXZ6EzEPmFef5gwuktoioqm/22FWiX+f9TGQu8jWA+Ny78cCG/qpLWZmg85ATjArgQmSGiW9EGgGlvRzm8zMBo0B20UWEdsk/SNwAzAEuDwiVvdzswYTdz3a7sy/zzpQRHS/lpmZWQ8N5C4yMzPrR04wZmZWCCeYAUxSg6SnJN2Vi1WcPkfSVyX9QdI/VahnvqTfS9o7vT9Y0toC2jszP9uCpLMlvbmvt2Nm9eEEM/D9NiKOgerT50TE54D/qlLPduBDxTaVmaldpDZ9OSJ+VvA2bTfQxc7Q5ZIelnRP2brd7Qx1SLpL0h2SXrsLbflW6f+FpC+WLftlT+sbzJxgBpfeTJ/zb8CnJT3vzENJn5O0UtKvJX0lF/+SpPsltUq6uvQHQdJH0vp3S/qepH0kvQ54B/DV9MfhiPTH4kRJb5G0KFfvCZJ+lF5PlbQs/TH5rqT9dvXLsX733M5QMh+YXr5SDTtDn0v1nA58s6eNiIgPR8S96e0Xy5a9rqf1DWZOMINLpelzxtRY9iHg58Ap+aCkqcAEsuR1DDBR0hslNQHvAl4NvBPIXzX9/Yh4TUS8CrgPmBURvyS7TulzEXFMRPw2t34rMFnSvun9e4BrJB0MnAm8OSKOBdqAz9T4eWw3FxG3Ap29qOJWYDyApM9Iuic9PpVi+0r6cdrRuUfSe1L8ZklNks4HhqUdnqvSsifS8zWS/q60obQz9C5JQ9IRVmmH66O9aP8eb8BeB2MVdTt9Tjf+lSwJ/DgXm5oed6b3+5ElnP2BayPiKYDSEUdytKR/AQ5M699QbaPpmqafAm+XtBh4K/B54HiyLrVfKJtQ6YXAsh58HhvY3g6skjQR+CDZXIQClku6BXgJsCEi3gogaXi+cEScLukfy46qShaS7ej8JF3IPQU4DZgFPBYRr0ljlr+QdGNEdBTzEXdvTjCDS6+mz4mI9tRH/u5cWMB5EbFTV4SkT1epaj4wMyLulvQB4IQaNn8NMIdsj3ZlRGxVllVaI+LkWj+DDQpflXQmsJnsD/4U4AcR8SSApO8DbwB+CnxN0gXAdRFxWw+2cT1wcUoi04FbI+KpdET/SkknpvWGk+1wDcoE4y6ywaUvps85F8gPrt4AfKg09iFpjKRDyLrT3i7pRWnZW3Nl9gc2StoLeF8uvjUtq+Rm4FjgI2TJBuBXwOsllbpB9pH00h5+Hht4St2sfxsR91D5yJ2I+A0wEVgFnCfpy7VuICKeJvtNTiM7klmYFgn4eNr+MRHRGBE39uKz7NGcYAaRiNgGlKbPuQ9Y1NPpc9L6d+Te3wh8B1gmaRWwGNg/IlaSJa+7ge+TjY88lop9CVhONrZyf676hcDnJN0p6Yiy7W4HriM7A+66FNsMfAC4WtKvyRLOy3vyeWxQuBWYmXZA9gX+HrhN0mHAnyLif4Cvke3AlHs27QhVspCs6+0N7OjmvQE4rVRG0ktzY4eDT0T4MUAfQANwTw/WPwv4pz7c/n7peR+yBHNsf38nfuyej0q/VeBqYCPwLFn37qzcsoq/VbLu1xMrxD8D3JMen0qxacCvgbvIju6bUvzm3OsLyHbGrkrvn8jVuRfwKPDtXOwFZGOVq9K2/hcY3t/fb389PBfZACZpHPBL4NGoPFCZX/erZHt2X4+IS/to+98hG4R/EbAgIs7ri3pt4JHUQDYOcnSN659F9sf+a0W2y3rHCcbM+l1/7wxZMZxgzMysEB7kNzOzQjjBmJlZIZxgzAokaXuaauSeNFfaPlXW/YCk/+hh/U2SLk6vT0hzupntFpxgzIr1VGQX3B0N/Bn4WF9VLGloRLRFxCdS6ATACcZ2G04wZvVzGzBe0khJP0yTIf5K0ivLV5T0dknL00WnP5N0aIqfJWmepBuBK9JRy3XpNN+Pkc14fZekNyibtr50wd8BktZWuWjQrM85wZjVgbLbHLyF7AK8rwB3RsQryaaDv6JCkZ8DkyPi1WRXjH8+t2wiMCMi3lsKRMRasinsL0pHTLeRXTBYmqKnGfheRDzbl5/LrBpPdmlWrGHacROt24DLyKbJeRdARNwk6aDymXzJJiK9RtJoslmi85MlLok0S3U3vkWWmH5INqXJR3b1Q5jtCicYs2I9VX7hYJoFulz5BWn/DlwYEUsknUA2NUrJk7VsOCJ+oexOkccDQyKb+NGsbtxFZlZ/t5JmkU7J45GIeLxsneHA79PrlhrrrTQb9RVkc3p9e1caatYbTjBm9XcW0JRmgD6fygnkLOC7km4DHqmx3h8Bf18a5E+xq4ARZEnGrK48VYzZAJZufDUjIk7pdmWzPuYxGLMBStK/k5259nfdrWtWBB/BmJlZITwGY2ZmhXCCMTOzQjjBmJlZIZxgzMysEE4wZmZWiP8DOjYRPy5sjDkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change [4] to [1] so the data is easier to understand.\n",
    "df['polarity'] = df['polarity'].replace(4,1)\n",
    "\n",
    "# Visualize whether data is evenly divided in terms of the polarity of tweets.\n",
    "ax = sns.countplot(x=\"polarity\", data=df, palette=['red', 'blue'])\n",
    "ax.set_title('Distribution in Polarity within Tweets')\n",
    "ax.set_xlabel('Polarity')\n",
    "ax.set_xticks([0,1])\n",
    "ax.set_xticklabels(['[0] Negative', '[1] Positive'])\n",
    "ax.set_ylabel('Number of Tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1600000 non-null  int64 \n",
      " 1   tweet     1600000 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Check for proper variable representations\n",
    "# Make all tweets lowercase for future processing\n",
    "df['tweet'] = df['tweet'].astype('str')\n",
    "df['tweet'] = df['tweet'].str.lower()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Text/Word Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section I used some of the most common text-preprocessing that are practiced\n",
    "in Natural Language Processing projects such as:     \n",
    "- Removing stop words     \n",
    "- Stemming words     \n",
    "- Lemmatizing words\n",
    "- Tokenizing individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>awww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save 50 rest go b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>need hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>hey long time see yes rain bit bit lol fine th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>nope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>que muera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>spring break plain city snowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>pierced ear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>bear watch thought ua loss embarrassing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>count idk either never talk anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>would first gun really though zac snyder douch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>wish got watch miss premiere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>hollis death scene hurt severely watch film wr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>file tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>ahh ive always wanted see rent love soundtrack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>oh dear drinking forgotten table drink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    polarity                                              tweet\n",
       "0          0       awww bummer shoulda got david carr third day\n",
       "1          0  upset update facebook texting might cry result...\n",
       "2          0  dived many time ball managed save 50 rest go b...\n",
       "3          0                    whole body feel itchy like fire\n",
       "4          0                                   behaving mad see\n",
       "5          0                                         whole crew\n",
       "6          0                                           need hug\n",
       "7          0  hey long time see yes rain bit bit lol fine th...\n",
       "8          0                                               nope\n",
       "9          0                                          que muera\n",
       "10         0                    spring break plain city snowing\n",
       "11         0                                        pierced ear\n",
       "12         0            bear watch thought ua loss embarrassing\n",
       "13         0                count idk either never talk anymore\n",
       "14         0  would first gun really though zac snyder douch...\n",
       "15         0                       wish got watch miss premiere\n",
       "16         0  hollis death scene hurt severely watch film wr...\n",
       "17         0                                           file tax\n",
       "18         0     ahh ive always wanted see rent love soundtrack\n",
       "19         0             oh dear drinking forgotten table drink"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============ 1.) Removing HTML Tags, URLs, HashTags, Email Addresses, & tagged usernames with RegEx\n",
    "\n",
    "# Parse through each of the tweets by using regular expressions to remove specific parts of the text\n",
    "# RegEx Reference: https://github.com/alvations/nltk/blob/develop/nltk/tokenize/casual.py#L122\n",
    "char_rm = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "hashtag, username=r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\", r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "emailAddr,htmlTags,=r\"\"\"[\\w.+-]+@[\\w-]+\\.(?:[\\w-]\\.?)+[\\w-]\"\"\", r\"\"\"<[^>\\s]+>\"\"\"\n",
    "urls=r\"\"\"(?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:[a-z]{2,13})/)(?:[^\\s()<>\n",
    "    {}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]\n",
    "    +\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:\n",
    "    [.\\-][a-z0-9]+)*[.](?:[a-z]{2,13})\\b/?(?!@))\"\"\"\n",
    "\n",
    "# Remove : hashtags, usernames, htmltags\n",
    "df['tweet'] = df['tweet'].replace(urls, \"\" ,regex=True)\n",
    "df['tweet'] = df['tweet'].replace(username, \"\" ,regex=True)\n",
    "df['tweet'] = df['tweet'].replace(hashtag, \"\" ,regex=True)\n",
    "df['tweet'] = df['tweet'].replace(htmlTags, \"\" ,regex=True)\n",
    "\n",
    "\n",
    "\n",
    "# ============ 2.) Tokenizing words, removing stop words, changing words back to their stem word and lemmatizing words\n",
    "\n",
    "# The following code was modeled after a snippet of code here:\n",
    "# https://www.traindex.io/blog/event-driven-data-pipelines-in-aws-480i/\n",
    "def preprocess(tweet, stem=False):\n",
    "    # Remove link, user and special characters\n",
    "    tweet = re.sub(char_rm, ' ', str(tweet)).strip()\n",
    "    tweet_token = word_tokenize(tweet)\n",
    "    tokens = []\n",
    "    for token in tweet_token:\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    finalwords=[]\n",
    "    for w in tokens:\n",
    "        if len(w)>1:\n",
    "            word = wordLemm.lemmatize(w)\n",
    "            finalwords.append(word)\n",
    "    return ' '.join(finalwords)\n",
    "\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(lambda x: preprocess(x))\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For my final dataset that I will be using for my classification problem, in regards of\n",
    "predicting whether a tweet is positive or negative, I selected to use only 2 columns:\n",
    "- Polarity : Whether a tweet in negative or positive ([0] negative; [1] Positive;)\n",
    "- Tweet : The text of a given tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Choosing a Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for metrics, for this specific classification problem, it is very easy to\n",
    "know that I will be using accuracy as a metric to evaluate my algorithms's \n",
    "performance.It will use binary cross entropy as it's loss function given\n",
    "the values of the polarity column are 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.4 Creating Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 259546 unique tokens. Distilled to 259546 top words.\n",
      "Shape of data tensor: (1600000, 1000)\n",
      "Shape of label tensor: (1600000, 2)\n",
      "259546\n",
      "======= Train Shapes ======\n",
      "Train : (1280000, 1000) \n",
      "Test  : (1280000, 2)\n",
      "\n",
      "====== Testing Shapes ======\n",
      "Train : (320000, 1000) \n",
      "Test  : (320000, 2)\n"
     ]
    }
   ],
   "source": [
    "# - Choose the method I'll use for dividing my data into training and testing (i.e., am I  using Stratified 10-fold cross validation? Shuffle splits? Why?).\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "MAX_TOP_WORDS = None \n",
    "MAX_REV_LEN = 1000   # maximum and minimum number of words\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(df.tweet)\n",
    "sequences = tokenizer.texts_to_sequences(df.tweet)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "MAX_TOP_WORDS = len(word_index) if MAX_TOP_WORDS==None else MAX_TOP_WORDS\n",
    "top_words = min((len(word_index),MAX_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_REV_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(df.polarity)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))\n",
    "\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe,test_size=0.2,stratify=df['polarity'],random_state=42)\n",
    "\n",
    "print(\"======= Train Shapes ======\")\n",
    "print('Train :', X_train.shape, '\\nTest  :', y_train_ohe.shape)\n",
    "print(\"\\n====== Testing Shapes ======\")\n",
    "print('Train :', X_test.shape, '\\nTest  :', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Explain why my chosen method is appropriate or use more than one method as appropriate.\n",
    "- Convince me that my cross validation method is a realistic mirroring of how an algorithm would be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "-----------------------------\n",
    "\n",
    "## 2. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Exploring Recurrent Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras libraries\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import average, Reshape, Input, Add, SimpleRNN\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, GRU\n",
    "from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, LSTM\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, concatenate\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph comparing Training & Validation Vs. Accuracy & Training loss\n",
    "def getChart(h):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.plot(h.history['accuracy'])\n",
    "    \n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.title('Training')\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.plot(h.history['val_accuracy'])\n",
    "    plt.title('Validation')\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.plot(h.history['loss'])\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.plot(h.history['val_loss'])\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "# Graph comparing Training & Validation Vs. Accuracy & Training loss\n",
    "def getCharts(h1, h2, h3, h4):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.subplot(2,2,1)\n",
    "    ax1 = sns.lineplot(data=h1.history['accuracy'], label='GRU - AdaM', color='blue')\n",
    "    sns.lineplot(data=h2.history['accuracy'], label='GRU - RMSProp', color='red')\n",
    "    sns.lineplot(data=h3.history['accuracy'], label='LSTM - AdaM', color='green')\n",
    "    sns.lineplot(data=h4.history['accuracy'], label='LSTM - RMSProp ', color='darkorange')\n",
    "    \n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.title('Training')\n",
    "    plt.subplot(2,2,2)\n",
    "    ax2 = sns.lineplot(data=h1.history['val_accuracy'], label='GRU - AdaM', color='blue')\n",
    "    sns.lineplot(data=h2.history['val_accuracy'], label='GRU - RMSProp', color='red')\n",
    "    sns.lineplot(data=h3.history['val_accuracy'], label='LSTM - AdaM', color='green')\n",
    "    sns.lineplot(data=h4.history['val_accuracy'], label='LSTM - RMSProp', color='darkorange')\n",
    "    plt.title('Validation')\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    ax3 = sns.lineplot(data=h1.history['loss'], label='GRU - AdaM', color='blue')\n",
    "    sns.lineplot(data=h2.history['loss'], label='GRU - RMSProp', color='red')\n",
    "    sns.lineplot(data=h3.history['loss'], label='LSTM - AdaM', color='green')\n",
    "    sns.lineplot(data=h4.history['loss'], label='LSTM - RMSProp', color='darkorange')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xlabel('epochs')\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    ax4 = sns.lineplot(data=h1.history['val_loss'], label='GRU - AdaM', color='blue')\n",
    "    sns.lineplot(data=h2.history['val_loss'], label='GRU - RMSProp', color='red')\n",
    "    sns.lineplot(data=h3.history['val_loss'], label='LSTM - AdaM', color='green')\n",
    "    sns.lineplot(data=h4.history['val_loss'], label='LSTM - RMSProp', color='darkorange')\n",
    "    plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1 Method 1: GRU Architecture\n",
    "\n",
    "A Gated Recurrent Network, or GRU\n",
    "- Be sure to use an embedding layer (pre-trained, from scratch, OR both).\n",
    "- Adjust hyper-parameters of the networks as needed to improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1000, 50)          12977300  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,023,001\n",
      "Trainable params: 13,023,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "######################## GRU Style Architecture : AdaM ##########################\n",
    "#################################################################################\n",
    "shpe = (MAX_REV_LEN,)\n",
    "embed_Size = 50\n",
    "rnn_StateSize = 100\n",
    "\n",
    "rnns = []\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "x1 = Embedding(MAX_TOP_WORDS,             # input dimension (max int of OHE)\n",
    "               embed_Size,                     # output dimension size\n",
    "               input_length=MAX_REV_LEN)(input_holder) # number of words in each sequence\n",
    "\n",
    "# create GRU\n",
    "x1 = GRU(rnn_StateSize, dropout=0.2, recurrent_dropout=0.2)(x1)\n",
    "x1 = Dense(1, activation='sigmoid')(x1)\n",
    "gru_1 = Model(inputs=input_holder,outputs=x1)\n",
    "\n",
    "print(gru_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:171 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((128, 1) vs (128, 2))\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1595 binary_crossentropy\n        K.binary_crossentropy(y_true, y_pred, from_logits=from_logits), axis=-1)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4692 binary_crossentropy\n        return nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n    /home/blurry/anaconda3/envs/ML/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py:171 sigmoid_cross_entropy_with_logits\n        raise ValueError(\"logits and labels must have the same shape (%s vs %s)\" %\n\n    ValueError: logits and labels must have the same shape ((128, 1) vs (128, 2))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "opt1 = Adam(lr=0.0001, epsilon=0.0001, clipnorm=1.0)\n",
    "gru_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_GRU1 = gru_1.fit(X_train, y_train_ohe, batch_size=128, \n",
    "                         epochs=50, verbose=1, validation_data=(X_test,y_test_ohe),\n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "\n",
    "yhat = np.round(gru_1.predict(X_test))\n",
    "print(mt.classification_report(y_test_ohe, yhat, zero_division=0))\n",
    "getChart(history_GRU1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to use an embedding layer (pre-trained, from scratch, OR both).\n",
    "# Adjust hyper-parameters of the networks as needed to improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1000)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 1000, 50)          12977300  \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               45600     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,023,001\n",
      "Trainable params: 13,023,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "######################## GRU Style Architecture : RMSProp #######################\n",
    "#################################################################################\n",
    "\n",
    "x2 = Embedding(MAX_TOP_WORDS,                  # input dimension (max int of OHE)\n",
    "               embed_Size,                     # output dimension size\n",
    "               input_length=MAX_REV_LEN)(input_holder) # number of words in each sequence\n",
    "\n",
    "\n",
    "# create GRU\n",
    "x2 = GRU(rnn_StateSize, dropout=0.2, recurrent_dropout=0.2)(x2)\n",
    "x2 = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "gru_2 = Model(inputs=input_holder,outputs=x1)\n",
    "\n",
    "print(gru_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "opt2 = Adam(lr=0.0001, epsilon=0.0001, clipnorm=1.0)\n",
    "gru_2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "history_GRU2 = gru_2.fit(X_train, y_train_ohe, batch_size=128,\n",
    "                         epochs=50, verbose=1, validation_data=(X_test,y_test_ohe),\n",
    "                         callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "\n",
    "yhat = np.round(gru_2.predict(X_test))\n",
    "print(mt.classification_report(y_test_ohe, yhat, zero_division=0))\n",
    "getChart(history_GRU2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to use an embedding layer (pre-trained, from scratch, OR both).\n",
    "# Adjust hyper-parameters of the networks as needed to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2 Method 2: LSTM  Architecture\n",
    "\n",
    "A \"long short-term memory\" units, or LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "######################## LSTM Style Architecture : AdaM ##########################\n",
    "##################################################################################\n",
    "shpe = (MAX_REV_LEN,)\n",
    "l2_lambda = 0.000001\n",
    "\n",
    "input_holder = Input(shape=shpe, dtype='int32')\n",
    "\n",
    "# Embedding Layer ================================================================\n",
    "x3 = Embedding(MAX_TOP_WORDS, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "               input_length=MAX_REV_LEN,\n",
    "               trainable=False)(input_holder)\n",
    "\n",
    "# Spatial Dropout Layer ==========================================================\n",
    "x3 = SpatialDropout1D(0.2)(x3)\n",
    "x3 = Activation('relu')(x3)\n",
    "\n",
    "# Convolution Layer ==============================================================\n",
    "x3 = Conv1D(64,5)(x3)\n",
    "\n",
    "# LSTM Layer =====================================================================\n",
    "x3 = LSTM(64, dropout=0.2, recurrent_dropout=0.2)(x3)\n",
    "\n",
    "# Bidirectional Layer ============================================================\n",
    "x3 = Bidirectional(x3)(x3)\n",
    "x3 = Activation(\"relu\")(x3)\n",
    "x3 = Dense(512)(x3)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x3 = Activation(\"relu\")(x3)\n",
    "x3 = Dropout(0.5)(x3)\n",
    "\n",
    "x3 = Activation(\"relu\")(x3)\n",
    "x3 = Dense(512)(x3)\n",
    "\n",
    "x3 = Activation(\"sigmoid\")(x3)\n",
    "x3 = Dense(1)(x3)\n",
    "\n",
    "lstm_1 = Model(inputs=input_holder,outputs=x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lstm_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history_LSTM1 = lstm_1.fit(X_train, y_train_ohe, batch_size=128,\n",
    "                           epochs=50, verbose=1, validation_data=(X_test,y_test_ohe),\n",
    "                           callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "\n",
    "yhat = np.round(lstm_1.predict(X_test))\n",
    "print(mt.classification_report(y_test_ohe, yhat, zero_division=0))\n",
    "getChart(history_LSTM1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to use an embedding layer (pre-trained, from scratch, OR both).\n",
    "# Adjust hyper-parameters of the networks as needed to improve generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "######################## LSTM Style Architecture : RMSProp #######################\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# - Using the best RNN parameters & architecture, add a second recurrent chain to my RNN.\n",
    "# - The input to the second chain should be the output sequence of the first chain.\n",
    "   \n",
    "# Embedding Layer ================================================================\n",
    "x4 = Embedding(input_dim=MAX_TOP_WORDS, output_dim=embedding_dim,\n",
    "               input_length=MAX_REV_LEN, weights=[embedding_matrix],\n",
    "               trainable=False)(x4)\n",
    "\n",
    "# Bidirectional Layer ============================================================ \n",
    "x4 = Bidirectional(LSTM(64,dropout=0.3,return_sequences=True)),\n",
    "x4 = Dropout(0.3),\n",
    "\n",
    "\n",
    "# Bidirectional Layer ============================================================ \n",
    "x4 = Bidirectional(LSTM(64,dropout=0.3))(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "\n",
    "x4 = Activation(\"relu\")(x4)\n",
    "x4 = Dense(512)(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "                  \n",
    "x4 = Activation(\"sigmoid\")(x4)\n",
    "x4 = Dense(1)(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "######################## LSTM Style Architecture : RMSProp #######################\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "# - Using the best RNN parameters & architecture, add a second recurrent chain to my RNN.\n",
    "# - The input to the second chain should be the output sequence of the first chain.\n",
    "   \n",
    "# Embedding Layer ================================================================\n",
    "x4 = Embedding(input_dim=MAX_TOP_WORDS, output_dim=embedding_dim,\n",
    "               input_length=MAX_REV_LEN, weights=[embedding_matrix],\n",
    "               trainable=False)(x4)\n",
    "\n",
    "# Bidirectional Layer ============================================================ \n",
    "x4 = Bidirectional(LSTM(64,dropout=0.3,return_sequences=True)),\n",
    "x4 = Dropout(0.3),\n",
    "\n",
    "\n",
    "# Bidirectional Layer ============================================================ \n",
    "x4 = Bidirectional(LSTM(64,dropout=0.3))(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "\n",
    "x4 = Activation(\"relu\")(x4)\n",
    "x4 = Dense(512)(x4)\n",
    "x4 = Dropout(0.3)(x4)\n",
    "                  \n",
    "x4 = Activation(\"sigmoid\")(x4)\n",
    "x4 = Dense(1)(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lstm_2.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history_LSTM2 = lstm_2.fit(X_train, y_train_ohe, batch_size=128,\n",
    "                           epochs=50, verbose=1, validation_data=(X_test,y_test_ohe),\n",
    "                           callbacks=[EarlyStopping(monitor='val_loss', patience=4)])\n",
    "\n",
    "yhat = np.round(lstm_2.predict(X_test))\n",
    "print(mt.classification_report(y_test_ohe, yhat, zero_division=0))\n",
    "getChart(history_LSTM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Be sure to use an embedding layer (pre-trained, from scratch, OR both).\n",
    "# Adjust hyper-parameters of the networks as needed to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- Discuss the performance of each network & compare them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Adding a 2nd recurrent chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Using the best RNN parameters & architecture, add a second recurrent chain to my RNN.\n",
    "# The input to the second chain should be the output sequence of the first chain.\n",
    "# Visualize the performance of training & validation sets versus the training iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# - Use the method of cross validation & evaluation criteria that I argued for at the beginning of the lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# - Visualize the results of all the RNNs I trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use proper statistical comparison techniques to determine which method(s) is (are) superior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "----------------------------------------\n",
    "\n",
    "### 3. t-SNE or RNN generating novel text\n",
    "You have free reign to provide additional analyses. Ideas:\n",
    "\n",
    "- Option 1: Use t-SNE (or SVD or PCA or UMAP) to visualize the word embeddings of a subset of words in your vocabulary.\n",
    "    - Try to interpret what each dimension reflects (in your own words).\n",
    "    - That is, try to explain what aspect of the language is encoded in the reduced dimensionality embedding.\n",
    "\n",
    "- Options 2: Use the ConceptNet Numberbatch embedding & compare to GloVe\n",
    "\n",
    "- Another Idea (NOT required): Try to create a RNN for generating novel text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
