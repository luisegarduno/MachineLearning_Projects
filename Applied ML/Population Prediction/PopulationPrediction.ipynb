{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optical-scheduling",
   "metadata": {},
   "source": [
    "# Population Prediction\n",
    "\n",
    "### Luis Garduno\n",
    "\n",
    "## 1. Business Understanding\n",
    "\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <u><code>About the datasets:</code></u>\n",
    "    \n",
    "    In order to generate the most accurate prediction of earth's population in 2122, it's\n",
    "    necessary to have population data for as many previous years as possible.\n",
    "    \n",
    "    The dataset that will be used will be a combination of 2 different datasets. The first\n",
    "    dataset contains the global population data from 1951 to 2020, whereas the second\n",
    "    dataset contains the each country's population data from 2021 to present time. \n",
    "\n",
    "-------------------------------------\n",
    "    \n",
    "Datasets [Kaggle]: \n",
    "1. [__World Population by Year (1951 - 2020)__](https://www.kaggle.com/sansuthi/world-population-by-year)\n",
    "2. [__World Population (2021 - Present)__](https://www.kaggle.com/rsrishav/world-population)\n",
    "3. [__International Database (IDB)__](https://www2.census.gov/programs-surveys/international-programs/about/idb/idbzip.zip)\n",
    "\n",
    "Question Of Interest : Predict the population of earth in 2122.\n",
    "    \n",
    "-------------------------------------\n",
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "lovely-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Dataset #1 (1951 - 2020) ***********\n",
      "\n",
      "--> Columns: ['Year' 'Population' 'ChangePerc' 'NetChange' 'Density' 'Urban'\n",
      " 'UrbanPerc']\n",
      "\n",
      "\n",
      "*********** Dataset #2 (2021 - Present) ***********\n",
      "\n",
      "--> Columns: ['iso_code' 'country' '2021_last_updated' '2020_population' 'area'\n",
      " 'density_sq_km' 'growth_rate' 'world_%' 'rank']\n",
      "*********** Dataset #3 (1951 - Present) ***********\n",
      "\n",
      "--> Columns: ['#YR' 'TFR' 'SRB' 'RNI' 'POP95_99' 'POP90_94' 'POP85_89' 'POP80_84'\n",
      " 'POP75_79' 'POP70_74' 'POP65_69' 'POP60_64' 'POP5_9' 'POP55_59'\n",
      " 'POP50_54' 'POP45_49' 'POP40_44' 'POP35_39' 'POP30_34' 'POP25_29'\n",
      " 'POP20_24' 'POP15_19' 'POP10_14' 'POP100_' 'POP0_4' 'POP' 'NMR' 'NAME'\n",
      " 'MR1_4' 'MR0_4' 'MPOP95_99' 'MPOP90_94' 'MPOP85_89' 'MPOP80_84'\n",
      " 'MPOP75_79' 'MPOP70_74' 'MPOP65_69' 'MPOP60_64' 'MPOP5_9' 'MPOP55_59'\n",
      " 'MPOP50_54' 'MPOP45_49' 'MPOP40_44' 'MPOP35_39' 'MPOP30_34' 'MPOP25_29'\n",
      " 'MPOP20_24' 'MPOP15_19' 'MPOP10_14' 'MPOP100_' 'MPOP0_4' 'MPOP' 'MMR1_4'\n",
      " 'MMR0_4' 'IMR_M' 'IMR_F' 'IMR' 'GRR' 'GR' 'FPOP95_99' 'FPOP90_94'\n",
      " 'FPOP85_89' 'FPOP80_84' 'FPOP75_79' 'FPOP70_74' 'FPOP65_69' 'FPOP60_64'\n",
      " 'FPOP5_9' 'FPOP55_59' 'FPOP50_54' 'FPOP45_49' 'FPOP40_44' 'FPOP35_39'\n",
      " 'FPOP30_34' 'FPOP25_29' 'FPOP20_24' 'FPOP15_19' 'FPOP10_14' 'FPOP100_'\n",
      " 'FPOP0_4' 'FPOP' 'FMR1_4' 'FMR0_4' 'GENC' 'FIPS' 'E0_M' 'E0_F' 'E0' 'CDR'\n",
      " 'CBR' 'ASFR45_49' 'ASFR40_44' 'ASFR35_39' 'ASFR30_34' 'ASFR25_29'\n",
      " 'ASFR20_24' 'ASFR15_19' 'AREA_KM2' 'POP_DENS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load each of the datasets into a dataframe\n",
    "\n",
    "print(\"*********** Dataset #1 (1951 - 2020) ***********\\n\")\n",
    "df_1 = pd.read_csv('https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/data/world-population_1.csv')\n",
    "print(\"--> Columns:\", df_1.columns.values)\n",
    "#print(df_1.info())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********** Dataset #2 (2021 - Present) ***********\\n\")\n",
    "df_2 = pd.read_csv('https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/data/world-population_2.csv')\n",
    "print(\"--> Columns:\", df_2.columns.values)\n",
    "#print(df_2.info())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"*********** Dataset #3 (1951 - Present) ***********\\n\")\n",
    "df_3 = pd.read_csv('https://raw.githubusercontent.com/luisegarduno/MachineLearning_Projects/master/data/idb5yr.all', delimiter='|', encoding='ISO-8859-1')\n",
    "print(\"--> Columns:\", df_3.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-memory",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "Printing out the information about the dataframe we are able to see that there are a\n",
    "total of 7 attributes in the first dataset, and 8 in the second.\n",
    "\n",
    "Attributes includes:\n",
    "- Description\n",
    "\n",
    "Below is a brief description of some of the key attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "expected-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: 7905336896\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>POP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2018</td>\n",
       "      <td>7597066210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2019</td>\n",
       "      <td>7676686052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2020</td>\n",
       "      <td>7756873419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2021</td>\n",
       "      <td>7831718605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2022</td>\n",
       "      <td>7905336896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR         POP\n",
       "67  2018  7597066210\n",
       "68  2019  7676686052\n",
       "69  2020  7756873419\n",
       "70  2021  7831718605\n",
       "71  2022  7905336896"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make year column easier to understand\n",
    "df_3.rename(columns={'#YR':'YEAR'}, inplace=True)\n",
    "\n",
    "# Remove every column except for year & population\n",
    "for col in df_3.columns.values:\n",
    "    if col != 'YEAR' and col != 'POP':\n",
    "        df_3.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Group by year & get sum\n",
    "df = df_3.groupby(by='YEAR')\n",
    "df = df['POP'].sum()\n",
    "\n",
    "# Finally create a new dataframe with new data\n",
    "pop_sum = []\n",
    "for i in range(1951, 2023):\n",
    "    pop_sum.append(df[i])\n",
    "df_pop = pd.DataFrame({'YEAR': list(range(1951, 2023)), 'POP': pop_sum})\n",
    "\n",
    "print(\"Current:\", df_pop['POP'][71])\n",
    "df_pop.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-roller",
   "metadata": {},
   "source": [
    "| Variable | Description | Type | Range |\n",
    "| -------- | ----------- | ---- | ----- |\n",
    "| A        | B           | C    | D     |   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-brisbane",
   "metadata": {},
   "source": [
    "The numbers above match exactly with what the numbers shown on the [__IDB web tool__](https://www.census.gov/data-tools/demo/idb/#/country?COUNTRY_YEAR=2022&COUNTRY_YR_ANIM=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-victorian",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2 Normalizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "tender-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total - 2020: 7697427912\n",
      "Total - 2021: 7817937695\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "\n",
    "total_2020 = 0\n",
    "total_2021 = 0\n",
    "for r in range(len(df_2)):\n",
    "    total_2020 += int(df_2['2020_population'][r].replace(',',''))\n",
    "    total_2021 += int(df_2['2021_last_updated'][r].replace(',',''))\n",
    "    #total += int(aye)\n",
    "    #total += df_2['2021_last_updated'][r]\n",
    "\n",
    "print(\"Total - 2020:\", total_2020)\n",
    "print(\"Total - 2021:\", total_2021)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-identifier",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3 Data Quality\n",
    "\n",
    "Using the `missingno` package, we are able to additionally confirm that all the data is complete and there is no missing entries with the dataset. If there was missing data, we could impute the missing values by using the k-nearest neighbor. But if an instance was missing a majority of its attributes, it would be removed from the dataset.\n",
    "\n",
    "The number of unique values in the column \" \" is printed to verify that all instances\n",
    "are weighted equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as mn\n",
    "\n",
    "mn.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-bangladesh",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.4 Cleaning the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-constitution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "\n",
    "# Given as to how\n",
    "\n",
    "\n",
    "\n",
    "del df['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-raising",
   "metadata": {},
   "source": [
    "\n",
    "-------------------\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.5 Creating Training & Test Data\n",
    "Using Scikit-learn's [cross-validation modules](https://scikit-learn.org/stable/modules/cross_validation.html) we are able to split our dataset for training and testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create X data & y target dataframe's\n",
    "y = df[''].values\n",
    "# del df['']\n",
    "X = df.to_numpy()\n",
    "\n",
    "\n",
    "# Divide the data: 80% Training & 20% Testing.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Training Set\", \"\\n   - Data Shape:\",X_train.shape,\"\\n   - Target Shape:\",y_train.shape)\n",
    "print(\"\\nTesting Set\",\"\\n   - Data Shape:\",X_test.shape ,\"\\n   - Target Shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-genetics",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "We perform a split within our dataset: 80% will be used for training, and 20% for testing. The 80/20 split is appropriate for\n",
    "the dataset because recall that the end goal is for users to be able to determine the probabilities of the earth's population 100 years from now.\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "## 3. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-console",
   "metadata": {},
   "source": [
    "\n",
    "----------------------\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.2 Custom Classifier Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-cyprus",
   "metadata": {},
   "source": [
    "\n",
    "------------------------\n",
    "\n",
    "## 4. Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-burlington",
   "metadata": {},
   "source": [
    "\n",
    "---------------------\n",
    "\n",
    "#### References\n",
    "\n",
    "Worldometer. World Population by Year. https://www.worldometers.info/world-population/world-population-by-year/ (Accessed 01-22-2022)\n",
    "\n",
    "Scikit-learn. Cross-validation. https://scikit-learn.org/stable/modules/cross_validation.html (Accessed 01-22-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-button",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
