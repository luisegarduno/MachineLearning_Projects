{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fa1c59-8399-4d20-86f1-814fbce8ce82",
   "metadata": {},
   "source": [
    "# Character RNNs: Generating Shakespearean Text\n",
    "\n",
    "### Luis G.\n",
    "\n",
    "Dataset : [Shakespear Dataset](https://homl.info/shakespeare) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd288e1b-5825-46e4-b6e2-d569d1ce6854",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "## 1. Preparation\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1287b585-13a9-48b8-aa36-bf009ea4405f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T03:48:03.669072682Z",
     "start_time": "2023-11-11T03:48:03.620452508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Download Shakespeare's dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "file_name = \"shakespeare.txt\"\n",
    "filepath = keras.utils.get_file(file_name, shakespeare_url)\n",
    "\n",
    "# Read/Store text file in string\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Print a small section of the dataset\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec78c6-2f78-4254-a6a8-276347374188",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Text/Word Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8d9cf6-1193-4d2c-8061-ed7419b569aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:02:09.185354277Z",
     "start_time": "2023-11-11T04:02:08.685629349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Characters: 39\n",
      "Dataset (characters) size: 1115394\n",
      "\n",
      "Sequence to text: [[20, 6, 9, 8, 3]]\n",
      "Text to sequence: ['f i r s t']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - encode each CHARACTER as an integer/id\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "\n",
    "# Number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "print(\"Distinct Characters:\", max_id)\n",
    "\n",
    "# Total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "print(\"Dataset (characters) size:\", dataset_size)\n",
    "\n",
    "# Verify tokenizer -> ex: word - \"First\"\n",
    "seq_to_txt = tokenizer.texts_to_sequences([\"First\"])\n",
    "txt_to_seq = tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
    "print(\"\\nSequence to text:\", seq_to_txt)\n",
    "print(\"Text to sequence:\", txt_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47480e4-dba0-4993-8471-fc6bcb0e44f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:02:12.422051124Z",
     "start_time": "2023-11-11T04:02:12.294949106Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the entire dataset so each char is represented by it's unique ID\n",
    "# - Subtract by 1, to get vals 0-38 vs. 1-39\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21910334-a202-4f35-ba7c-225da976be9e",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Creating Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82274f13-24b6-4df7-adf1-b2922bc94c52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T04:02:27.901881634Z",
     "start_time": "2023-11-11T04:02:27.898214347Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 01:59:02.171970: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.175778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.175950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.177070: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 01:59:02.178280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.178415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.178516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.603082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.603221: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.603320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 01:59:02.603407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7341 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Training set = 90%\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30236a7-9ed0-4ed5-9f65-50924e74c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pre) Window length\n",
    "n_steps = 100\n",
    "\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1,drop_remainder=True)\n",
    "\n",
    "# Preview of the dataset (e.g., the first 3 elements)\n",
    "preview_nested = dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "844697b0-9bcd-4514-bdc1-0b1d30d4cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nested dataset to flat dataset (doesn't contain datasets)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "preview_flat = dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4468d5e5-ba44-4ddf-9517-f43a8c870840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before] Nested Dataset\n",
      "[19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0]\n",
      "[5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4]\n",
      "[8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4, 8]\n",
      "\n",
      "\n",
      " --------------------------------------- \n",
      "\n",
      "[After] Flattening Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 01:59:18.281763: W tensorflow/core/framework/dataset.cc:768] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "        19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "         9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0,\n",
       "        14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7,\n",
       "        22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,\n",
       "         7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0]),\n",
       " array([ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,\n",
       "         3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,  9,\n",
       "        15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0, 14,\n",
       "         1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7, 22,\n",
       "         1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,  7,\n",
       "         2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0,  4]),\n",
       " array([ 8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,  3,\n",
       "         8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,  9, 15,\n",
       "         0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0, 14,  1,\n",
       "         0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7, 22,  1,\n",
       "         4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,  7,  2,\n",
       "         0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0,  4,  8])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"[Before] Nested Dataset:\")\n",
    "for window in preview_nested:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "    \n",
    "print(\"\\n\\n --------------------------------------- \\n\")\n",
    "\n",
    "print(\"[After] Flattening Dataset:\")\n",
    "list(preview_flat.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a943a6-74ee-498d-a139-d0f9d5c7e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32    \n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff22c6f-0dea-40b3-bd53-dabfdd1a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each char using one-hot vector since there's only 39 distinct characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Add prefetching to dataset\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250d061-4079-452d-82c9-c42059b51701",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\n",
    "## 2. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Exploring Recurrent Network Architectures: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c05a8-871f-4c08-9ed4-c71cb8237ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, TimeDistributed, Dense\n",
    "\n",
    "model = Sequential([\n",
    "            GRU(128, return_sequences=True, input_shape=[None,max_id], dropout=0.2),\n",
    "            GRU(128, return_sequences=True, dropout=0.2),\n",
    "            TimeDistributed(Dense(max_id, activation=\"softmax\"))\n",
    "        ])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset,epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
