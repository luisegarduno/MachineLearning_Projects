{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fa1c59-8399-4d20-86f1-814fbce8ce82",
   "metadata": {},
   "source": [
    "# Character RNNs: Generating Shakespearean Text\n",
    "\n",
    "### Luis G.\n",
    "\n",
    "Dataset : [Shakespear Dataset](https://homl.info/shakespeare) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd288e1b-5825-46e4-b6e2-d569d1ce6854",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "## 1. Preparation\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1287b585-13a9-48b8-aa36-bf009ea4405f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:01.705716435Z",
     "start_time": "2023-11-11T07:53:01.703122564Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Download Shakespeare's dataset\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"\n",
    "file_name = \"shakespeare.txt\"\n",
    "filepath = keras.utils.get_file(file_name, shakespeare_url)\n",
    "\n",
    "# Read/Store text file in string\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n",
    "\n",
    "# Print a small section of the dataset\n",
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dec78c6-2f78-4254-a6a8-276347374188",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.2 Text/Word Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca8d9cf6-1193-4d2c-8061-ed7419b569aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:04.497660729Z",
     "start_time": "2023-11-11T07:53:04.023686353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Characters: 39\n",
      "Dataset (characters) size: 1115394\n",
      "\n",
      "Sequence to text: [[20, 6, 9, 8, 3]]\n",
      "Text to sequence: ['f i r s t']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize - encode each CHARACTER as an integer/id\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)\n",
    "\n",
    "# Number of distinct characters\n",
    "max_id = len(tokenizer.word_index)\n",
    "print(\"Distinct Characters:\", max_id)\n",
    "\n",
    "# Total number of characters\n",
    "dataset_size = tokenizer.document_count\n",
    "print(\"Dataset (characters) size:\", dataset_size)\n",
    "\n",
    "# Verify tokenizer -> ex: word - \"First\"\n",
    "seq_to_txt = tokenizer.texts_to_sequences([\"First\"])\n",
    "txt_to_seq = tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n",
    "print(\"\\nSequence to text:\", seq_to_txt)\n",
    "print(\"Text to sequence:\", txt_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47480e4-dba0-4993-8471-fc6bcb0e44f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:05.795269571Z",
     "start_time": "2023-11-11T07:53:05.700576796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode the entire dataset so each char is represented by it's unique ID\n",
    "# - Subtract by 1, to get vals 0-38 vs. 1-39\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21910334-a202-4f35-ba7c-225da976be9e",
   "metadata": {},
   "source": [
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.3 Creating Training & Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82274f13-24b6-4df7-adf1-b2922bc94c52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:07.131075872Z",
     "start_time": "2023-11-11T07:53:07.129471256Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 05:12:56.683533: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:56.686885: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:56.687001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:56.687565: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 05:12:56.688495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:56.688610: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:56.688701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:57.021381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:57.021518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:57.021617: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-11-11 05:12:57.021704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7297 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Training set = 90%\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30236a7-9ed0-4ed5-9f65-50924e74c80b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:15.063232752Z",
     "start_time": "2023-11-11T07:53:15.002923874Z"
    }
   },
   "outputs": [],
   "source": [
    "# (Pre) Window length\n",
    "n_steps = 100\n",
    "\n",
    "# target = input shifted 1 character ahead\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1,drop_remainder=True)\n",
    "\n",
    "# Store 3 elements for the (nested) dataset (see below)\n",
    "preview_nested = dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844697b0-9bcd-4514-bdc1-0b1d30d4cf13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:20.545854435Z",
     "start_time": "2023-11-11T07:53:20.507743674Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert nested dataset to flat dataset (doesn't contain datasets)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "# Store 3 elements for the (flat) dataset (see below)\n",
    "preview_flat = dataset.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4468d5e5-ba44-4ddf-9517-f43a8c870840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-11T07:53:24.929912350Z",
     "start_time": "2023-11-11T07:53:24.846461227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== [Before] Nested Dataset ==================================\n",
      "[19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0]\n",
      "[5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4]\n",
      "[8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 21, 1, 19, 3, 8, 1, 0, 16, 1, 0, 22, 8, 3, 18, 1, 1, 12, 0, 4, 9, 15, 0, 19, 13, 8, 2, 6, 1, 8, 17, 0, 6, 1, 4, 8, 0, 14, 1, 0, 7, 22, 1, 4, 24, 26, 10, 10, 4, 11, 11, 23, 10, 7, 22, 1, 4, 24, 17, 0, 7, 22, 1, 4, 24, 26, 10, 10, 19, 5, 8, 7, 2, 0, 18, 5, 2, 5, 35, 1, 9, 23, 10, 15, 3, 13, 0, 4, 8]\n",
      "\n",
      "================================== [After] Flattening Dataset ==================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 05:12:57.148299: W tensorflow/core/framework/dataset.cc:768] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "        19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "         9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0,\n",
       "        14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7,\n",
       "        22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,\n",
       "         7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0]),\n",
       " array([ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,\n",
       "         3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,  9,\n",
       "        15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0, 14,\n",
       "         1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7, 22,\n",
       "         1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,  7,\n",
       "         2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0,  4]),\n",
       " array([ 8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,  3,\n",
       "         8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,  9, 15,\n",
       "         0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0, 14,  1,\n",
       "         0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,  7, 22,  1,\n",
       "         4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,  5,  8,  7,  2,\n",
       "         0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3, 13,  0,  4,  8])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"================================== [Before] Nested Dataset ==================================\")\n",
    "for window in preview_nested:\n",
    "    print(list(window.as_numpy_iterator()))\n",
    "    \n",
    "print(\"\\n================================== [After] Flattening Dataset ==================================\")\n",
    "list(preview_flat.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7371a8f1-5378-4df9-a357-ba7ce386e586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fdc734-e94f-463a-b9e4-f5fcaaa56d45",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "Here is a quick breakdown/demo on what just happened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb1a494-982d-4548-9db6-d5b07698ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== Preview dataset post shuffle + batch ==================================\n",
      "[array([[19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,\n",
      "         1, 19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,\n",
      "         0,  4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,\n",
      "         4,  8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11,\n",
      "        11, 23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26,\n",
      "        10, 10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23,\n",
      "        10, 15,  3, 13,  0],\n",
      "       [ 8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,\n",
      "         3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
      "         9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,\n",
      "         0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23,\n",
      "        10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10,\n",
      "        19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,\n",
      "         3, 13,  0,  4,  8],\n",
      "       [ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
      "        19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,\n",
      "         4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,\n",
      "         8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11,\n",
      "        23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10,\n",
      "        10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10,\n",
      "        15,  3, 13,  0,  4]])]\n",
      "\n",
      "================================== Preview dataset post map ==================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([[ 8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,\n",
       "           3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "           9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,\n",
       "           0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23,\n",
       "          10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10,\n",
       "          19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,\n",
       "           3, 13,  0,  4],\n",
       "         [19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,\n",
       "           1, 19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,\n",
       "           0,  4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,\n",
       "           4,  8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11,\n",
       "          11, 23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26,\n",
       "          10, 10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23,\n",
       "          10, 15,  3, 13],\n",
       "         [ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "          19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,\n",
       "           4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,\n",
       "           8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11,\n",
       "          23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10,\n",
       "          10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10,\n",
       "          15,  3, 13,  0]]),\n",
       "  array([[ 7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,  3,\n",
       "           8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,  9,\n",
       "          15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,  0,\n",
       "          14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23, 10,\n",
       "           7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10, 19,\n",
       "           5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,  3,\n",
       "          13,  0,  4,  8],\n",
       "         [ 5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1,\n",
       "          19,  3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,\n",
       "           4,  9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,\n",
       "           8,  0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11,\n",
       "          23, 10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10,\n",
       "          10, 19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10,\n",
       "          15,  3, 13,  0],\n",
       "         [ 8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 21,  1, 19,\n",
       "           3,  8,  1,  0, 16,  1,  0, 22,  8,  3, 18,  1,  1, 12,  0,  4,\n",
       "           9, 15,  0, 19, 13,  8,  2,  6,  1,  8, 17,  0,  6,  1,  4,  8,\n",
       "           0, 14,  1,  0,  7, 22,  1,  4, 24, 26, 10, 10,  4, 11, 11, 23,\n",
       "          10,  7, 22,  1,  4, 24, 17,  0,  7, 22,  1,  4, 24, 26, 10, 10,\n",
       "          19,  5,  8,  7,  2,  0, 18,  5,  2,  5, 35,  1,  9, 23, 10, 15,\n",
       "           3, 13,  0,  4]]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_dataset = preview_flat.shuffle(10000).batch(batch_size)\n",
    "print(\"================================== Preview dataset post shuffle + batch ==================================\")\n",
    "print(list(preview_dataset.as_numpy_iterator()))\n",
    "\n",
    "preview_dataset = preview_dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "print(\"\\n================================== Preview dataset post map ==================================\")\n",
    "list(preview_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab8cb95-c2ad-4d9f-8c4c-8fc2b79e36d3",
   "metadata": {},
   "source": [
    "Here are some [definitions](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that may further help:\n",
    "* `shuffle()`: Randomly shuffles the elements of this dataset.\n",
    "* `batch()`: Combines consecutive elements of this dataset into batches.\n",
    "* `map()`: This transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input.\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff22c6f-0dea-40b3-bd53-dabfdd1a7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each char using one-hot vector since there's only 39 distinct characters\n",
    "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "# Add prefetching to dataset\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250d061-4079-452d-82c9-c42059b51701",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "\n",
    "## 2. Modeling\n",
    "\n",
    "### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.1 Exploring Recurrent Network Architectures: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5c05a8-871f-4c08-9ed4-c71cb8237ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-11 05:17:09.411842: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.6188\n",
      "Epoch 2/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.5394\n",
      "Epoch 3/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.5187\n",
      "Epoch 4/20\n",
      "31368/31368 [==============================] - 204s 6ms/step - loss: 1.5074\n",
      "Epoch 5/20\n",
      "31368/31368 [==============================] - 204s 7ms/step - loss: 1.5003\n",
      "Epoch 6/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.4952\n",
      "Epoch 7/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.4901\n",
      "Epoch 8/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.4863\n",
      "Epoch 9/20\n",
      "31368/31368 [==============================] - 203s 6ms/step - loss: 1.4837\n",
      "Epoch 10/20\n",
      "31368/31368 [==============================] - 211s 7ms/step - loss: 1.4807\n",
      "Epoch 11/20\n",
      "31368/31368 [==============================] - 214s 7ms/step - loss: 1.4786\n",
      "Epoch 12/20\n",
      "31368/31368 [==============================] - 214s 7ms/step - loss: 1.4766\n",
      "Epoch 13/20\n",
      "31368/31368 [==============================] - 213s 7ms/step - loss: 1.4749\n",
      "Epoch 14/20\n",
      "31368/31368 [==============================] - 213s 7ms/step - loss: 1.4737\n",
      "Epoch 15/20\n",
      "31368/31368 [==============================] - 214s 7ms/step - loss: 1.4725\n",
      "Epoch 16/20\n",
      "31368/31368 [==============================] - 214s 7ms/step - loss: 1.4711\n",
      "Epoch 17/20\n",
      "31368/31368 [==============================] - 212s 7ms/step - loss: 1.4704\n",
      "Epoch 18/20\n",
      "31368/31368 [==============================] - 212s 7ms/step - loss: 1.4694\n",
      "Epoch 19/20\n",
      "31368/31368 [==============================] - 214s 7ms/step - loss: 1.4684\n",
      "Epoch 20/20\n",
      "31368/31368 [==============================] - 212s 7ms/step - loss: 1.4676\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, TimeDistributed, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(128, return_sequences=True, input_shape=[None,max_id], dropout=0.2))\n",
    "model.add(GRU(128, return_sequences=True, dropout=0.2))\n",
    "model.add(TimeDistributed(Dense(max_id, activation=\"softmax\")))\n",
    "\n",
    "#model = Sequential([\n",
    "#            GRU(128, return_sequences=True, input_shape=[None,max_id], dropout=0.2),\n",
    "#            GRU(128, return_sequences=True, dropout=0.2),\n",
    "#            TimeDistributed(Dense(max_id, activation=\"softmax\"))\n",
    "#        ])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf6e24d5-25c9-4e6a-81a6-6b9d30e01466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_2 (GRU)                 (None, None, 128)         64896     \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, None, 128)         99072     \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 39)         5031      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,999\n",
      "Trainable params: 168,999\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08eade91-4fd2-4620-8bc3-bb3bc5e23848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, show_layer_names=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3a34146-5830-428c-af8e-c4999a9fe87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6096d3cf-a245-4a24-ab47-a2d612fb45b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = np.argmax(model(X_new), axis=-1)\n",
    "\n",
    "# 1st sentence, last character\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6441f-f5b9-4260-8d8a-c57c83e7b14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
